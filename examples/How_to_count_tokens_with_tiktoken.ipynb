{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# 如何使用tiktoken计算标记数\n", "\n", "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md) 是由OpenAI开发的快速开源分词器。\n", "\n", "给定一个文本字符串（例如，`\"tiktoken is great!\"`）和一个编码（例如，`\"cl100k_base\"`），分词器可以将文本字符串拆分为标记列表（例如，`[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`）。\n", "\n", "将文本字符串拆分为标记对于GPT模型很有用，因为这些模型以标记的形式看待文本。知道文本字符串中有多少标记可以告诉您（a）该字符串是否过长而无法被文本模型处理，以及（b）OpenAI API调用的成本（因为使用是按标记计费的）。\n", "\n", "## 编码\n", "\n", "编码指定了文本如何转换为标记。不同的模型使用不同的编码。\n", "\n", "`tiktoken` 支持OpenAI模型使用的三种编码：\n", "\n", "| 编码名称               | OpenAI模型                                       |\n", "|-------------------------|-----------------------------------------------------|\n", "| `cl100k_base`           | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`  |\n", "| `p50k_base`             | Codex模型, `text-davinci-002`, `text-davinci-003`|\n", "| `r50k_base` (或 `gpt2`) | GPT-3模型如 `davinci`                         |\n", "\n", "您可以使用以下方式通过 `tiktoken.encoding_for_model()` 获取模型的编码：\n", "```python\n", "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n", "```\n", "\n", "请注意，`p50k_base` 与 `r50k_base` 有很大重叠，在非代码应用中，它们通常会给出相同的标记。\n", "\n", "## 根据语言的分词器库\n", "\n", "对于 `cl100k_base` 和 `p50k_base` 编码：\n", "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n", "- .NET / C#: [SharpToken](https://github.com/dmitry-brazhenko/SharpToken), [TiktokenSharp](https://github.com/aiqinxuancai/TiktokenSharp)\n", "- Java: [jtokkit](https://github.com/knuddelsgmbh/jtokkit)\n", "- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\n", "- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\n", "\n", "对于 `r50k_base` (`gpt2`) 编码，分词器在许多语言中都可用。\n", "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)（或者 [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast)）\n", "- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n", "- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n", "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n", "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n", "- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\n", "- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\n", "\n", "（OpenAI不对第三方库作任何认可或保证。）\n", "\n", "## 典型的字符串如何被分词\n", "\n", "在英语中，标记通常的长度范围从一个字符到一个单词（例如，`\"t\"` 或 `\" great\"`），尽管在一些语言中，标记可以比一个字符更短或比一个单词更长。空格通常与单词的开头分组（例如，`\" is\"` 而不是 `\"is \"` 或 `\" \"`+`\"is\"`）。您可以快速检查字符串如何被分词在 [OpenAI Tokenizer](https://beta.openai.com/tokenizer) 或第三方 [Tiktokenizer](https://tiktokenizer.vercel.app/) 网页应用中。\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 0. 安装 `tiktoken`\n", "\n", "如果需要，可以使用 `pip` 安装 `tiktoken`：\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install --upgrade tiktoken\n", "%pip install --upgrade openai\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 1. 导入 `tiktoken`\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import tiktoken\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 2. 加载一个编码\n", "\n", "使用 `tiktoken.get_encoding()` 按名称加载一个编码。\n", "\n", "第一次运行时，需要互联网连接进行下载。之后的运行将不需要互联网连接。\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["encoding = tiktoken.get_encoding(\"cl100k_base\")\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["使用 `tiktoken.encoding_for_model()` 函数可以自动加载给定模型名称的正确编码。\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 3. 使用`encoding.encode()`将文本转换为标记符号\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["`.encode()` 方法将一个文本字符串转换为一个令牌整数列表。\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": ["[83, 1609, 5963, 374, 2294, 0]"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["encoding.encode(\"tiktoken is great!\")\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["通过计算`.encode()`返回的列表的长度来计算令牌的数量。\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["def num_tokens_from_string(string: str, encoding_name: str) -> int:\n", "    \"\"\"返回文本字符串中的标记数。\"\"\"\n", "    encoding = tiktoken.get_encoding(encoding_name)\n", "    num_tokens = len(encoding.encode(string))\n", "    return num_tokens\n", "\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": ["6"]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 4. 使用`encoding.decode()`将标记转换为文本\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["`.decode()`将一个令牌整数列表转换为字符串。\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": ["'tiktoken is great!'"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["encoding.decode([83, 1609, 5963, 374, 2294, 0])\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["警告：虽然`.decode()`可以应用于单个标记，但要注意对于不在utf-8边界上的标记，可能会有信息丢失。\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["对于单个令牌，`.decode_single_token_bytes()` 可以安全地将单个整数令牌转换为其表示的字节。\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["[b't', b'ik', b'token', b' is', b' great', b'!']"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["（在字符串前面的`b`表示这些字符串是字节字符串。）\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 5. 比较编码方式\n", "\n", "不同的编码方式在分割单词、处理空格和非英文字符方面有所不同。利用上面的方法，我们可以比较几个示例字符串在不同编码方式下的表现。\n"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["def compare_encodings(example_string: str) -> None:\n", "    \"\"\"打印三种字符串编码的比较结果。\"\"\"\n", "    # 打印示例字符串\n", "    print(f'\\nExample string: \"{example_string}\"')\n", "    # 对于每种编码，请输出其标记数量、标记整数以及标记字节。\n", "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\"]:\n", "        encoding = tiktoken.get_encoding(encoding_name)\n", "        token_integers = encoding.encode(example_string)\n", "        num_tokens = len(token_integers)\n", "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n", "        print()\n", "        print(f\"{encoding_name}: {num_tokens} tokens\")\n", "        print(f\"token integers: {token_integers}\")\n", "        print(f\"token bytes: {token_bytes}\")\n", "        \n"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Example string: \"antidisestablishmentarianism\"\n", "\n", "r50k_base: 5 tokens\n", "token integers: [415, 29207, 44390, 3699, 1042]\n", "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n", "\n", "p50k_base: 5 tokens\n", "token integers: [415, 29207, 44390, 3699, 1042]\n", "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n", "\n", "cl100k_base: 6 tokens\n", "token integers: [519, 85342, 34500, 479, 8997, 2191]\n", "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n"]}], "source": ["compare_encodings(\"antidisestablishmentarianism\")\n", "\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Example string: \"2 + 2 = 4\"\n", "\n", "r50k_base: 5 tokens\n", "token integers: [17, 1343, 362, 796, 604]\n", "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n", "\n", "p50k_base: 5 tokens\n", "token integers: [17, 1343, 362, 796, 604]\n", "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n", "\n", "cl100k_base: 7 tokens\n", "token integers: [17, 489, 220, 17, 284, 220, 19]\n", "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"]}], "source": ["compare_encodings(\"2 + 2 = 4\")\n", "\n"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Example string: \"お誕生日おめでとう\"\n", "\n", "r50k_base: 14 tokens\n", "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n", "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n", "\n", "p50k_base: 14 tokens\n", "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n", "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n", "\n", "cl100k_base: 9 tokens\n", "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n", "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"]}], "source": ["compare_encodings(\"お誕生日おめでとう\")\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 6. 统计用于聊天完成API调用的标记\n", "\n", "ChatGPT模型如`gpt-3.5-turbo`和`gpt-4`与旧的完成模型一样使用标记，但由于其基于消息的格式，更难以计算对话中将使用多少标记。\n", "\n", "以下是一个用于计算传递给`gpt-3.5-turbo`或`gpt-4`的消息标记数量的示例函数。\n", "\n", "请注意，从消息中计算标记的确切方式可能因模型而异。请将下面函数中的计数视为估计，而非永恒的保证。\n", "\n", "特别是，使用可选功能输入的请求将消耗额外的标记，这些额外的标记不包括下面计算的估计值。\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n", "    \"\"\"返回由消息列表使用的令牌数量。\"\"\"\n", "    try:\n", "        encoding = tiktoken.encoding_for_model(model)\n", "    except KeyError:\n", "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n", "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n", "    if model in {\n", "        \"gpt-3.5-turbo-0613\",\n", "        \"gpt-3.5-turbo-16k-0613\",\n", "        \"gpt-4-0314\",\n", "        \"gpt-4-32k-0314\",\n", "        \"gpt-4-0613\",\n", "        \"gpt-4-32k-0613\",\n", "        }:\n", "        tokens_per_message = 3\n", "        tokens_per_name = 1\n", "    elif model == \"gpt-3.5-turbo-0301\":\n", "        tokens_per_message = 4  # 每条消息都遵循以下格式：<|start|>{角色/名称}\\n{内容}<|end|>\\n\n", "        tokens_per_name = -1  # 如果存在名称，则角色会被省略。\n", "    elif \"gpt-3.5-turbo\" in model:\n", "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n", "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n", "    elif \"gpt-4\" in model:\n", "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n", "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n", "    else:\n", "        raise NotImplementedError(\n", "            f\"\"\"num_tokens_from_messages() 函数尚未针对模型 {model} 实现。\"\"\"\n", "        )\n", "    num_tokens = 0\n", "    for message in messages:\n", "        num_tokens += tokens_per_message\n", "        for key, value in message.items():\n", "            num_tokens += len(encoding.encode(value))\n", "            if key == \"name\":\n", "                num_tokens += tokens_per_name\n", "    num_tokens += 3  # 每条回复都以<|start|>assistant<|message|>作为开头。\n", "    return num_tokens\n", "\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["gpt-3.5-turbo-0301\n", "127 prompt tokens counted by num_tokens_from_messages().\n", "127 prompt tokens counted by the OpenAI API.\n", "\n", "gpt-3.5-turbo-0613\n", "129 prompt tokens counted by num_tokens_from_messages().\n", "129 prompt tokens counted by the OpenAI API.\n", "\n", "gpt-3.5-turbo\n", "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n", "129 prompt tokens counted by num_tokens_from_messages().\n", "129 prompt tokens counted by the OpenAI API.\n", "\n", "gpt-4-0314\n", "129 prompt tokens counted by num_tokens_from_messages().\n", "129 prompt tokens counted by the OpenAI API.\n", "\n", "gpt-4-0613\n", "129 prompt tokens counted by num_tokens_from_messages().\n", "129 prompt tokens counted by the OpenAI API.\n", "\n", "gpt-4\n", "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n", "129 prompt tokens counted by num_tokens_from_messages().\n", "129 prompt tokens counted by the OpenAI API.\n", "\n"]}], "source": ["# let's verify the function above matches the OpenAI API response\n", "\n", "from openai import OpenAI\n", "import os\n", "\n", "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n", "\n", "example_messages = [\n", "    {\n", "        \"role\": \"system\",\n", "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n", "    },\n", "    {\n", "        \"role\": \"system\",\n", "        \"name\": \"example_user\",\n", "        \"content\": \"New synergies will help drive top-line growth.\",\n", "    },\n", "    {\n", "        \"role\": \"system\",\n", "        \"name\": \"example_assistant\",\n", "        \"content\": \"Things working well together will increase revenue.\",\n", "    },\n", "    {\n", "        \"role\": \"system\",\n", "        \"name\": \"example_user\",\n", "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n", "    },\n", "    {\n", "        \"role\": \"system\",\n", "        \"name\": \"example_assistant\",\n", "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n", "    },\n", "    {\n", "        \"role\": \"user\",\n", "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n", "    },\n", "]\n", "\n", "for model in [\n", "    \"gpt-3.5-turbo-0301\",\n", "    \"gpt-3.5-turbo-0613\",\n", "    \"gpt-3.5-turbo\",\n", "    \"gpt-4-0314\",\n", "    \"gpt-4-0613\",\n", "    \"gpt-4\",\n", "    ]:\n", "    print(model)\n", "    # example token count from the function defined above\n", "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n", "    # example token count from the OpenAI API\n", "    response = client.chat.completions.create(model=model,\n", "    messages=example_messages,\n", "    temperature=0,\n", "    max_tokens=1)\n", "    print(f'{response.usage.prompt_tokens} prompt tokens counted by the OpenAI API.')\n", "    print()\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.5"}, "vscode": {"interpreter": {"hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"}}}, "nbformat": 4, "nbformat_minor": 2}