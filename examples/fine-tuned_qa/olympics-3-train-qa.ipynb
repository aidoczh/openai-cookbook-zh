{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<span style=\"color:orange; font-weight:bold\">注意：为了回答基于文本文档的问题，我们建议使用<a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\">使用嵌入进行问答</a>中的步骤。以下部分的一些代码可能依赖于<a href=\"https://github.com/openai/openai-cookbook/tree/main/transition_guides_for_deprecated_API_endpoints\">已弃用的API端点</a>。</span>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. 训练一个专门用于问答的微调模型\n", "本笔记本将利用上下文、问题和答案对的数据集，另外创建对抗性问题和上下文对，其中问题并非在该上下文中生成。在这些情况下，模型将被提示回答“没有足够的上下文来回答问题”。我们还将训练一个鉴别器模型，该模型根据上下文预测问题是否可以被回答。\n", "\n", "我们还将添加一些困难的对抗性示例，这些示例将基于语义上相似的部分或来自同一篇文章的相邻部分。\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>title</th>\n", "      <th>heading</th>\n", "      <th>content</th>\n", "      <th>tokens</th>\n", "      <th>context</th>\n", "      <th>questions</th>\n", "      <th>answers</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>2020 Summer Olympics</td>\n", "      <td>Summary</td>\n", "      <td>The 2020 Summer Olympics (Japanese: 2020年夏季オリン...</td>\n", "      <td>713</td>\n", "      <td>2020 Summer Olympics\\nSummary\\n\\nThe 2020 Summ...</td>\n", "      <td>1. What is the 2020 Summer Olympics?\\n2. When ...</td>\n", "      <td>1. The 2020 Summer Olympics is an internationa...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2020 Summer Olympics</td>\n", "      <td>Host city selection</td>\n", "      <td>The International Olympic Committee (IOC) vote...</td>\n", "      <td>126</td>\n", "      <td>2020 Summer Olympics\\nHost city selection\\n\\nT...</td>\n", "      <td>1. \\n2. \\n3. \\n4.</td>\n", "      <td>1. What is the International Olympic Committee...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2020 Summer Olympics</td>\n", "      <td>Impact of the COVID-19 pandemic</td>\n", "      <td>In January 2020, concerns were raised about th...</td>\n", "      <td>369</td>\n", "      <td>2020 Summer Olympics\\nImpact of the COVID-19 p...</td>\n", "      <td>1. What was the COVID-19 pandemic?\\n2. How did...</td>\n", "      <td>1. The COVID-19 pandemic was a pandemic that o...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>2020 Summer Olympics</td>\n", "      <td>Qualifying event cancellation and postponement</td>\n", "      <td>Concerns about the pandemic began to affect qu...</td>\n", "      <td>298</td>\n", "      <td>2020 Summer Olympics\\nQualifying event cancell...</td>\n", "      <td>1. What was the original location of the Asia ...</td>\n", "      <td>1. The original location of the Asia &amp; Oceania...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>2020 Summer Olympics</td>\n", "      <td>Effect on doping tests</td>\n", "      <td>Mandatory doping tests were being severely res...</td>\n", "      <td>163</td>\n", "      <td>2020 Summer Olympics\\nEffect on doping tests\\n...</td>\n", "      <td>1. What was the COVID-19 pandemic?\\n2. What di...</td>\n", "      <td>1. The COVID-19 pandemic was a pandemic that o...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                  title                                         heading  \\\n", "0  2020 Summer Olympics                                         Summary   \n", "1  2020 Summer Olympics                             Host city selection   \n", "2  2020 Summer Olympics                 Impact of the COVID-19 pandemic   \n", "3  2020 Summer Olympics  Qualifying event cancellation and postponement   \n", "4  2020 Summer Olympics                          Effect on doping tests   \n", "\n", "                                             content  tokens  \\\n", "0  The 2020 Summer Olympics (Japanese: 2020年夏季オリン...     713   \n", "1  The International Olympic Committee (IOC) vote...     126   \n", "2  In January 2020, concerns were raised about th...     369   \n", "3  Concerns about the pandemic began to affect qu...     298   \n", "4  Mandatory doping tests were being severely res...     163   \n", "\n", "                                             context  \\\n", "0  2020 Summer Olympics\\nSummary\\n\\nThe 2020 Summ...   \n", "1  2020 Summer Olympics\\nHost city selection\\n\\nT...   \n", "2  2020 Summer Olympics\\nImpact of the COVID-19 p...   \n", "3  2020 Summer Olympics\\nQualifying event cancell...   \n", "4  2020 Summer Olympics\\nEffect on doping tests\\n...   \n", "\n", "                                           questions  \\\n", "0  1. What is the 2020 Summer Olympics?\\n2. When ...   \n", "1                                 1. \\n2. \\n3. \\n4.    \n", "2  1. What was the COVID-19 pandemic?\\n2. How did...   \n", "3  1. What was the original location of the Asia ...   \n", "4  1. What was the COVID-19 pandemic?\\n2. What di...   \n", "\n", "                                             answers  \n", "0  1. The 2020 Summer Olympics is an internationa...  \n", "1  1. What is the International Olympic Committee...  \n", "2  1. The COVID-19 pandemic was a pandemic that o...  \n", "3  1. The original location of the Asia & Oceania...  \n", "4  1. The COVID-19 pandemic was a pandemic that o...  "]}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": ["import openai\n", "import pandas as pd\n", "df = pd.read_csv('olympics-data/olympics_qa.csv')\n", "olympics_search_fileid = \"file-c3shd8wqF3vSCKaukW4Jr1TT\"\n", "df.head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["将数据集分割为训练集和测试集\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": ["(3014, 754)"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["from sklearn.model_selection import train_test_split\n", "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n", "len(train_df), len(test_df)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们检查我们打算使用的分隔符是否不存在于上下文中。\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": ["0"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["df.context.str.contains('->').sum()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.1 为问答和鉴别器模型创建微调数据集\n", "微调数据集是按以下方式创建的。对于每个相应的问题、答案和上下文对，我们创建：\n", "- 正例：正确的问题、答案、上下文对\n", "- 负例：\n", "  - 随机负例，其中随机上下文与问题配对\n", "  - 两个困难负例\n", "    - 一个来自相同的维基百科文章\n", "    - 另一个是与正确上下文最相似的\n", "\n", "这个过程是有噪声的，因为有时候给定不同的上下文可能也可以回答问题，但平均而言，我们希望这不会太大影响性能。\n", "\n", "我们对鉴别器模型和问答模型都应用相同的数据集创建过程。我们分别对训练集和测试集应用该过程，以确保训练集中的示例不会出现在测试集中。\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["import random\n", "\n", "def get_random_similar_contexts(question, context, file_id=olympics_search_fileid, search_model='ada', max_rerank=10):\n", "    \"\"\"\n", "    利用搜索文件，寻找与给定上下文相似的语境。\n", "    \"\"\"\n", "    try:\n", "        # 待办事项：openai.Engine(search_model) 已被弃用。\n", "        results = openai.Engine(search_model).search(\n", "            search_model=search_model, \n", "            query=question, \n", "            max_rerank=max_rerank,\n", "            file=file_id\n", "        )\n", "        candidates = []\n", "        for result in results['data'][:3]:\n", "            if result['text'] == context:\n", "                continue\n", "            candidates.append(result['text'])\n", "        random_candidate = random.choice(candidates)\n", "        return random_candidate\n", "    except Exception as e:\n", "        print(e)\n", "        return \"\"\n", "\n", "def create_fine_tuning_dataset(df, discriminator=False, n_negative=1, add_related=False):\n", "    \"\"\"\n", "    Create a dataset for fine tuning the OpenAI model; either for a discriminator model, \n", "    or a model specializing in Q&A, where it says if no relevant context is found.\n", "\n", "    Parameters\n", "    ----------\n", "    df: pd.DataFrame\n", "        The dataframe containing the question, answer and context pairs\n", "    discriminator: bool\n", "        Whether to create a dataset for the discriminator\n", "    n_negative: int\n", "        The number of random negative samples to add (using a random context)\n", "    add_related: bool\n", "        Whether to add the related contexts to the correct context. These are hard negative examples\n", "\n", "    Returns\n", "    -------\n", "    pd.DataFrame\n", "        The dataframe containing the prompts and completions, ready for fine-tuning\n", "    \"\"\"\n", "    rows = []\n", "    for i, row in df.iterrows():\n", "        for q, a in zip((\"1.\" + row.questions).split('\\n'), (\"1.\" + row.answers).split('\\n')):\n", "            if len(q) >10 and len(a) >10:\n", "                if discriminator:\n", "                    rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" yes\"})\n", "                else:\n", "                    rows.append({\"prompt\":f\"{row.context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" {a[2:].strip()}\"})\n", "\n", "    for i, row in df.iterrows():\n", "        for q in (\"1.\" + row.questions).split('\\n'):\n", "            if len(q) >10:\n", "                for j in range(n_negative + (2 if add_related else 0)):\n", "                    random_context = \"\"\n", "                    if j == 0 and add_related:\n", "                        # 根据源自同一维基百科页面的相关上下文进行添加\n", "                        subset = df[(df.title == row.title) & (df.context != row.context)]\n", "                        \n", "                        if len(subset) < 1:\n", "                            continue\n", "                        random_context = subset.sample(1).iloc[0].context\n", "                    if j == 1 and add_related:\n", "                        # 根据搜索结果中最相似的上下文，添加相关内容。\n", "                        random_context = get_random_similar_contexts(q[2:].strip(), row.context, search_model='ada', max_rerank=10)\n", "                    else:\n", "                        while True:\n", "                            # 添加随机上下文，但这并不是正确的上下文。\n", "                            random_context = df.sample(1).iloc[0].context\n", "                            if random_context != row.context:\n", "                                break\n", "                    if discriminator:\n", "                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" no\"})\n", "                    else:\n", "                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" No appropriate context found to answer the question.\"})\n", "\n", "    return pd.DataFrame(rows) \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们对鉴别器和问答模型的数据集创建过程采用相同的方法。我们分别对训练集和测试集应用该过程，以确保训练集中的示例不会出现在测试集中。\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": []}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["for name, is_disc in [('discriminator', True), ('qa', False)]:\n", "    for train_test, dt in [('train', train_df), ('test', test_df)]:\n", "        ft = create_fine_tuning_dataset(dt, discriminator=is_disc, n_negative=1, add_related=True)\n", "        ft.to_json(f'{name}_{train_test}.jsonl', orient='records', lines=True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们根据微调工具的建议对数据进行了格式化，可以使用以下命令进行操作：\n", "> openai tools fine_tunes.prepare_data -f qa_train.jsonl\n", "\n", "我们强烈建议您使用这个工具，它会为微调提供数据格式方面的改进建议。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.2 提交数据集进行微调\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": []}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["!openai api fine_tunes.create -t \"olympics-data/discriminator_train.jsonl\" -v \"olympics-data/discriminator_test.jsonl\" --batch_size 16  --compute_classification_metrics --classification_positive_class \" yes\" --model ada\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": []}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["!openai api fine_tunes.create -t \"olympics-data/qa_train.jsonl\" -v \"olympics-data/qa_test.jsonl\" --batch_size 16\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.3 使用微调后的模型\n", "\n", "现在我们将使用微调后的鉴别器和微调后的问答模型。通过请求logprobs，我们可以看到鉴别器在“是”和“否”答案中的确定程度。\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": ["[<OpenAIObject at 0x7fe812e602b0> JSON: {\n", "   \" no\": -10.819577,\n", "   \" yes\": -2.045765e-05\n", " }]"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["ft_discriminator = \"curie:ft-openai-internal-2021-08-23-23-58-57\"\n", "ft_qa = \"curie:ft-openai-internal-2021-08-23-17-54-10\"\n", "\n", "def apply_ft_discriminator(context, question, discriminator_model):\n", "    \"\"\"\n", "    将微调后的判别器应用于一个问题，以评估该问题是否能从上下文中得到解答。\n", "    \"\"\"\n", "    prompt = f\"{context}\\nQuestion: {question}\\n Related:\"\n", "    result = openai.chat.completions.create(model=discriminator_model, prompt=prompt, max_tokens=1, temperature=0, top_p=1, n=1, logprobs=2)\n", "    return result['choices'][0]['logprobs']['top_logprobs']\n", "\n", "apply_ft_discriminator('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n", "                        'What was the first human-made object in space?', ft_discriminator)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以看到模型能够很好地泛化到不同的上下文和问题中。\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["' The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957'"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["def apply_ft_qa_answer(context, question, answering_model):\n", "    \"\"\"\n", "    将微调后的判别器应用于一个问题\n", "    \"\"\"\n", "    prompt = f\"{context}\\nQuestion: {question}\\nAnswer:\"\n", "    result = openai.chat.completions.create(model=answering_model, prompt=prompt, max_tokens=30, temperature=0, top_p=1, n=1, stop=['.','\\n'])\n", "    return result['choices'][0]['text']\n", "\n", "apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n", "                    'What was the first human-made object in space?', ft_qa)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以看到，当上下文合适时，模型可以回答问题。\n"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": ["' The Soviet Union was the first country to successfully launch a satellite into space'"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',\n", "                    'What is impressive about the Soviet Union?', ft_qa)\n"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": ["' No appropriate context found to answer the question'"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',\n", "                    'How many cars were produced in the Soviet Union in 1970?', ft_qa)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以看到，模型知道何时回答问题，何时表示存在不足的上下文无法回答问题。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们还可以将鉴别器和基础模型，或者微调的问答模型结合起来。鉴别器本质上可以作为一个决策，判断在给定的上下文中是否可以回答问题。\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": ["' Weather could cause a sport event to have no crowd'"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["def answer_question_conditionally(answering_model, discriminator_model, context, question, discriminator_logprob_yes_modifier=0):\n", "    logprobs = apply_ft_discriminator(context, question, discriminator_model)\n", "    yes_logprob = logprobs[' yes'] if ' yes' in logprobs else -100\n", "    no_logprob = logprobs[' no'] if ' no' in logprobs else -100\n", "    if yes_logprob + discriminator_logprob_yes_modifier < no_logprob:\n", "        return \" No appropriate context found to answer the question based on the discriminator.\"\n", "    return apply_ft_qa_answer(context, question, answering_model)\n", "answer_question_conditionally(ft_qa, ft_discriminator, \n", "                                \"Crowdless games are a rare although not unheard-of occurrence in sports. \\\n", "                                 When they do occur, it is usually the result of events beyond the control \\\n", "                                 of the teams or fans, such as weather-related concerns, public health concerns, \\\n", "                                 or wider civil disturbances unrelated to the game. For instance, \\\n", "                                 the COVID-19 pandemic caused many sports leagues around the world \\\n", "                                 to be played behind closed doors.\",\n", "                                \"Could weather cause a sport event to have no crowd?\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["上面的函数演示了如何可能将鉴别器和经过精细调整的问答模型结合起来。这样可以更精细地控制我们希望模型在回答问题之前变得多么确定。\n", "\n", "现在让我们来看一下答案端点的工作原理 - 结合搜索从知识库中检索相关内容，然后使用经过精细调整的问答模型来回答问题。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.4 根据知识库回答问题\n", "最后，我们可以使用类似于[/answers](https://beta.openai.com/docs/api-reference/answers)端点的逻辑，首先搜索相关的上下文，然后请求一个问答模型根据该上下文回答问题。如果您想查看实现细节，请查看[`answers_with_ft.py`](answers_with_ft.py)文件。\n"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/plain": ["\" Canada won the Women's football tournament at the 2020 Olympic games\""]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": ["from answers_with_ft import answer_question\n", "answer_question(olympics_search_fileid, ft_qa, \"Which country won the Women's football tournament at the 2020 Olympic games?\")\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3.9.9 64-bit ('3.9.9')", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}, "orig_nbformat": 4, "vscode": {"interpreter": {"hash": "cb9817b186a29e4e9713184d901f26c1ee05ad25243d878baff7f31bb1fef480"}}}, "nbformat": 4, "nbformat_minor": 2}