{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Azure音频低语（预览）示例\n", "\n", "该示例展示了如何使用Azure OpenAI Whisper模型来转录音频文件。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 设置\n", "\n", "首先，我们安装必要的依赖项并导入我们将使用的库。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["! pip install \"openai>=1.0.0,<2.0.0\"\n", "! pip install python-dotenv\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import openai\n", "import dotenv\n", "\n", "dotenv.load_dotenv()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 认证\n", "\n", "Azure OpenAI 服务支持多种认证机制，包括 API 密钥和 Azure Active Directory 令牌凭据。\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["use_azure_active_directory = False  # 将此标志设置为 True，如果您正在使用 Azure Active Directory。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 使用API密钥进行身份验证\n", "\n", "要设置OpenAI SDK以使用*Azure API密钥*，我们需要将`api_key`设置为与您的端点关联的密钥（您可以在[Azure门户](https://portal.azure.com)的*\"资源管理\"*下的*\"密钥和端点\"*中找到此密钥）。您还将在此处找到您资源的端点。\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["if not use_azure_active_directory:\n", "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n", "    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n", "\n", "    client = openai.AzureOpenAI(\n", "        azure_endpoint=endpoint,\n", "        api_key=api_key,\n", "        api_version=\"2023-09-01-preview\"\n", "    )\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 使用Azure Active Directory进行身份验证\n", "现在让我们看看如何通过Azure Active Directory进行身份验证。我们将从安装`azure-identity`库开始。该库将提供我们需要进行身份验证的令牌凭据，并通过`get_bearer_token_provider`辅助函数帮助我们构建一个令牌凭据提供程序。建议使用`get_bearer_token_provider`而不是向`AzureOpenAI`提供静态令牌，因为这个API会自动为您缓存和刷新令牌。\n", "\n", "有关如何设置Azure Active Directory身份验证与Azure OpenAI的更多信息，请参阅[文档](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity)。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["! pip install \"azure-identity>=1.15.0\"\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n", "\n", "if use_azure_active_directory:\n", "    endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n", "    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n", "\n", "    client = openai.AzureOpenAI(\n", "        azure_endpoint=endpoint,\n", "        azure_ad_token_provider=get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"),\n", "        api_version=\"2023-09-01-preview\"\n", "    )\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> 注意：如果未提供以下参数，则AzureOpenAI将从其对应的环境变量中推断出来：\n", "\n", "- `api_key` 从 `AZURE_OPENAI_API_KEY`\n", "- `azure_ad_token` 从 `AZURE_OPENAI_AD_TOKEN`\n", "- `api_version` 从 `OPENAI_API_VERSION`\n", "- `azure_endpoint` 从 `AZURE_OPENAI_ENDPOINT`\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 部署\n", "\n", "在本节中，我们将使用`whisper-1`模型创建一个部署，用于转录音频文件。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 部署：在Azure OpenAI Studio中创建\n", "让我们部署一个模型以供whisper使用。前往 https://portal.azure.com，找到您的Azure OpenAI资源，然后导航到Azure OpenAI Studio。点击“部署”选项卡，然后为您想要用于whisper的模型创建一个部署。您为模型提供的部署名称将在下面的代码中使用。\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["deployment = \"whisper-deployment\" # 在此处填写从门户获取的部署名称\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 音频转录\n", "\n", "音频转录，或称为语音转文本，是将口语转换为文本的过程。使用`openai.Audio.transcribe`方法将音频文件流转录为文本。\n", "\n", "您可以从[GitHub上的Azure AI Speech SDK存储库](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles)获取示例音频文件。\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# 下载示例音频文件\n", "import requests\n", "\n", "sample_audio_url = \"https://github.com/Azure-Samples/cognitive-services-speech-sdk/raw/master/sampledata/audiofiles/wikipediaOcelot.wav\"\n", "audio_file = requests.get(sample_audio_url)\n", "with open(\"wikipediaOcelot.wav\", \"wb\") as f:\n", "    f.write(audio_file.content)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["transcription = client.audio.transcriptions.create(\n", "    file=open(\"wikipediaOcelot.wav\", \"rb\"),\n", "    model=deployment,\n", ")\n", "print(transcription.text)\n"]}], "metadata": {"kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.0"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}