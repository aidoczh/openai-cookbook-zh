{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# ä½¿ç”¨logprobsè¿›è¡Œåˆ†ç±»å’Œé—®ç­”è¯„ä¼°\n", "\n", "æœ¬ç¬”è®°æ¼”ç¤ºäº†åœ¨Chat Completions APIä¸­ä½¿ç”¨`logprobs`å‚æ•°çš„æ–¹æ³•ã€‚å½“å¯ç”¨`logprobs`æ—¶ï¼ŒAPIä¼šè¿”å›æ¯ä¸ªè¾“å‡ºtokençš„å¯¹æ•°æ¦‚ç‡ï¼Œä»¥åŠåœ¨æ¯ä¸ªtokenä½ç½®ä¸Šæœ€å¯èƒ½çš„æœ‰é™æ•°é‡çš„tokenåŠå…¶å¯¹æ•°æ¦‚ç‡ã€‚ç›¸å…³çš„è¯·æ±‚å‚æ•°åŒ…æ‹¬ï¼š\n", "* `logprobs`ï¼šæ˜¯å¦è¿”å›è¾“å‡ºtokençš„å¯¹æ•°æ¦‚ç‡ã€‚å¦‚æœä¸ºtrueï¼Œåˆ™è¿”å›æ¶ˆæ¯å†…å®¹ä¸­æ¯ä¸ªè¾“å‡ºtokençš„å¯¹æ•°æ¦‚ç‡ã€‚ç›®å‰åœ¨`gpt-4-vision-preview`æ¨¡å‹ä¸Šä¸å¯ç”¨ã€‚\n", "* `top_logprobs`ï¼šä¸€ä¸ªä»‹äº0å’Œ5ä¹‹é—´çš„æ•´æ•°ï¼ŒæŒ‡å®šè¦åœ¨æ¯ä¸ªtokenä½ç½®è¿”å›çš„æœ€å¯èƒ½tokençš„æ•°é‡ï¼Œæ¯ä¸ªtokenéƒ½æœ‰ä¸€ä¸ªå…³è”çš„å¯¹æ•°æ¦‚ç‡ã€‚å¦‚æœä½¿ç”¨è¯¥å‚æ•°ï¼Œ`logprobs`å¿…é¡»è®¾ç½®ä¸ºtrueã€‚\n", "\n", "è¾“å‡ºtokençš„å¯¹æ•°æ¦‚ç‡è¡¨ç¤ºåœ¨ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹æ¯ä¸ªtokenå‡ºç°åœ¨åºåˆ—ä¸­çš„å¯èƒ½æ€§ã€‚ç®€å•æ¥è¯´ï¼Œå¯¹æ•°æ¦‚ç‡æ˜¯`log(p)`ï¼Œå…¶ä¸­`p` = æ ¹æ®ä¸Šä¸‹æ–‡ä¸­çš„å…ˆå‰tokenåœ¨ç‰¹å®šä½ç½®å‡ºç°çš„tokençš„æ¦‚ç‡ã€‚å…³äº`logprobs`çš„ä¸€äº›å…³é”®ç‚¹ï¼š\n", "* è¾ƒé«˜çš„å¯¹æ•°æ¦‚ç‡è¡¨æ˜åœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­è¯¥tokençš„å¯èƒ½æ€§è¾ƒé«˜ã€‚è¿™ä½¿ç”¨æˆ·å¯ä»¥è¯„ä¼°æ¨¡å‹å¯¹å…¶è¾“å‡ºçš„ä¿¡å¿ƒæˆ–æ¢ç´¢æ¨¡å‹è€ƒè™‘çš„æ›¿ä»£å“åº”ã€‚\n", "* å¯¹æ•°æ¦‚ç‡å¯ä»¥æ˜¯ä»»ä½•è´Ÿæ•°æˆ–`0.0`ã€‚`0.0`å¯¹åº”äº100%çš„æ¦‚ç‡ã€‚\n", "* å¯¹æ•°æ¦‚ç‡ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®¡ç®—åºåˆ—çš„è”åˆæ¦‚ç‡ï¼Œä½œä¸ºå„ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ä¹‹å’Œã€‚è¿™å¯¹äºè¯„åˆ†å’Œæ’åæ¨¡å‹è¾“å‡ºå¾ˆæœ‰ç”¨ã€‚å¦ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å–å¥å­çš„æ¯ä¸ªtokençš„å¹³å‡å¯¹æ•°æ¦‚ç‡æ¥é€‰æ‹©æœ€ä½³ç”Ÿæˆã€‚\n", "* æˆ‘ä»¬å¯ä»¥æ£€æŸ¥åˆ†é…ç»™ä¸åŒå€™é€‰tokençš„`logprobs`ï¼Œä»¥äº†è§£æ¨¡å‹è€ƒè™‘çš„å“ªäº›é€‰é¡¹æ˜¯å¯ä¿¡çš„æˆ–ä¸å¯ä¿¡çš„ã€‚\n", "\n", "è™½ç„¶`logprobs`æœ‰å„ç§ç”¨ä¾‹ï¼Œä½†æœ¬ç¬”è®°å°†é‡ç‚¹ä»‹ç»å…¶ç”¨äºä»¥ä¸‹æ–¹é¢ï¼š\n", "\n", "1. åˆ†ç±»ä»»åŠ¡\n", "\n", "* å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å‡†ç¡®è¡¡é‡æ¨¡å‹å¯¹å…¶è¾“å‡ºçš„ä¿¡å¿ƒå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚`logprobs`ä¸ºæ¯ä¸ªç±»åˆ«é¢„æµ‹æä¾›äº†ä¸€ä¸ªæ¦‚ç‡ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè®¾ç½®è‡ªå·±çš„åˆ†ç±»æˆ–ç½®ä¿¡é˜ˆå€¼ã€‚\n", "\n", "2. æ£€ç´¢ï¼ˆé—®ç­”ï¼‰è¯„ä¼°\n", "\n", "* `logprobs`å¯ä»¥å¸®åŠ©åœ¨æ£€ç´¢åº”ç”¨ä¸­è¿›è¡Œè‡ªæˆ‘è¯„ä¼°ã€‚åœ¨é—®ç­”ç¤ºä¾‹ä¸­ï¼Œæ¨¡å‹è¾“å‡ºä¸€ä¸ªè™šæ„çš„`has_sufficient_context_for_answer`å¸ƒå°”å€¼ï¼Œå¯ä»¥ä½œä¸ºç­”æ¡ˆæ˜¯å¦åŒ…å«åœ¨æ£€ç´¢å†…å®¹ä¸­çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚è¿™ç§ç±»å‹çš„è¯„ä¼°å¯ä»¥å‡å°‘åŸºäºæ£€ç´¢çš„å¹»è§‰ï¼Œå¹¶æé«˜å‡†ç¡®æ€§ã€‚\n", "\n", "3. è‡ªåŠ¨å®Œæˆ\n", "* `logprobs`å¯ä»¥å¸®åŠ©æˆ‘ä»¬å†³å®šåœ¨ç”¨æˆ·è¾“å…¥æ—¶å¦‚ä½•å»ºè®®å•è¯ã€‚\n", "\n", "4. Tokené«˜äº®æ˜¾ç¤ºå’Œè¾“å‡ºå­—èŠ‚\n", "* ç”¨æˆ·å¯ä»¥è½»æ¾åœ°ä½¿ç”¨å¯ç”¨`logprobs`æ—¶é™„å¸¦çš„å†…ç½®æ ‡è®°åŒ–åˆ›å»ºä¸€ä¸ªæ ‡è®°é«˜äº®å™¨ã€‚æ­¤å¤–ï¼Œå­—èŠ‚å‚æ•°åŒ…æ‹¬æ¯ä¸ªè¾“å‡ºå­—ç¬¦çš„ASCIIç¼–ç ï¼Œè¿™å¯¹äºå†ç°è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ç‰¹åˆ«æœ‰ç”¨ã€‚\n", "\n", "5. è®¡ç®—å›°æƒ‘åº¦\n", "* `logprobs`å¯ç”¨äºå¸®åŠ©æˆ‘ä»¬è¯„ä¼°æ¨¡å‹å¯¹ç»“æœçš„æ•´ä½“ä¿¡å¿ƒï¼Œå¹¶å¸®åŠ©æˆ‘ä»¬æ¯”è¾ƒæ¥è‡ªä¸åŒæç¤ºçš„ç»“æœçš„ç½®ä¿¡åº¦ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0. å¯¼å…¥å’Œå·¥å…·\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from openai import OpenAI\n", "from math import exp\n", "import numpy as np\n", "from IPython.display import display, HTML\n", "import os\n", "\n", "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["def get_completion(\n", "    messages: list[dict[str, str]],\n", "    model: str = \"gpt-4\",\n", "    max_tokens=500,\n", "    temperature=0,\n", "    stop=None,\n", "    seed=123,\n", "    tools=None,\n", "    logprobs=None,  # æ˜¯å¦è¿”å›è¾“å‡ºæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡ã€‚å¦‚æœä¸ºçœŸï¼Œåˆ™åœ¨æ¶ˆæ¯å†…å®¹ä¸­è¿”å›æ¯ä¸ªè¾“å‡ºæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡ã€‚\n", "    top_logprobs=None,\n", ") -> str:\n", "    params = {\n", "        \"model\": model,\n", "        \"messages\": messages,\n", "        \"max_tokens\": max_tokens,\n", "        \"temperature\": temperature,\n", "        \"stop\": stop,\n", "        \"seed\": seed,\n", "        \"logprobs\": logprobs,\n", "        \"top_logprobs\": top_logprobs,\n", "    }\n", "    if tools:\n", "        params[\"tools\"] = tools\n", "\n", "    completion = client.chat.completions.create(**params)\n", "    return completion\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. ä½¿ç”¨`logprobs`æ¥è¯„ä¼°åˆ†ç±»ä»»åŠ¡çš„ç½®ä¿¡åº¦\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å‡è®¾æˆ‘ä»¬æƒ³è¦åˆ›å»ºä¸€ä¸ªç³»ç»Ÿï¼Œå°†æ–°é—»æ–‡ç« åˆ†ç±»åˆ°ä¸€ç»„é¢„å®šä¹‰çš„ç±»åˆ«ä¸­ã€‚å¦‚æœæ²¡æœ‰å¯ç”¨`logprobs`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¯¹è¯å®Œæˆæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä½†è¦è¯„ä¼°æ¨¡å‹å¯¹å…¶åˆ†ç±»çš„ç¡®å®šæ€§è¦å›°éš¾å¾—å¤šã€‚\n", "\n", "ç°åœ¨ï¼Œå¯ç”¨äº†`logprobs`ï¼Œæˆ‘ä»¬å¯ä»¥å‡†ç¡®åœ°çœ‹åˆ°æ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ä¿¡å¿ƒç¨‹åº¦ï¼Œè¿™å¯¹äºåˆ›å»ºå‡†ç¡®å¯ä¿¡çš„åˆ†ç±»å™¨è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‰€é€‰ç±»åˆ«çš„å¯¹æ•°æ¦‚ç‡å¾ˆé«˜ï¼Œè¿™è¡¨æ˜æ¨¡å‹å¯¹å…¶åˆ†ç±»éå¸¸æœ‰ä¿¡å¿ƒã€‚å¦‚æœå¾ˆä½ï¼Œåˆ™è¡¨æ˜æ¨¡å‹çš„ä¿¡å¿ƒè¾ƒä½ã€‚è¿™åœ¨æ¨¡å‹çš„åˆ†ç±»ä¸ç¬¦åˆé¢„æœŸæˆ–éœ€è¦äººå·¥å®¡æ ¸æˆ–éªŒè¯æ¨¡å‹è¾“å‡ºçš„æƒ…å†µä¸‹ç‰¹åˆ«æœ‰ç”¨ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["æˆ‘ä»¬å°†ä»ä¸€ä¸ªæç¤ºå¼€å§‹ï¼Œè¯¥æç¤ºå‘æ¨¡å‹å±•ç¤ºå››ä¸ªç±»åˆ«ï¼š**æŠ€æœ¯ï¼Œæ”¿æ²»ï¼Œä½“è‚²å’Œè‰ºæœ¯**ã€‚ç„¶åï¼Œæ¨¡å‹è¢«è¦æ±‚ä»…æ ¹æ®æ–‡ç« æ ‡é¢˜å°†æ–‡ç« åˆ†ç±»åˆ°è¿™äº›ç±»åˆ«ä¸­ã€‚\n"]}, {"cell_type": "code", "execution_count": 266, "metadata": {}, "outputs": [], "source": ["CLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article.\n", "Classify the article into one of the following categories: Technology, Politics, Sports, and Art.\n", "Return only the name of the category, and nothing else.\n", "MAKE SURE your output is one of the four categories stated.\n", "Article headline: {headline}\"\"\"\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["è®©æˆ‘ä»¬çœ‹çœ‹ä¸‰ä¸ªæ ·æœ¬æ ‡é¢˜ï¼Œå¹¶é¦–å…ˆä»ä¸€ä¸ªæ²¡æœ‰`logprobs`çš„æ ‡å‡†Chat Completionsè¾“å‡ºå¼€å§‹ã€‚\n"]}, {"cell_type": "code", "execution_count": 267, "metadata": {}, "outputs": [], "source": ["headlines = [\n", "    \"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n", "    \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n", "    \"Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\",\n", "]\n", "\n"]}, {"cell_type": "code", "execution_count": 268, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n", "Category: Technology\n", "\n", "\n", "Headline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n", "Category: Politics\n", "\n", "\n", "Headline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n", "Category: Art\n", "\n"]}], "source": ["for headline in headlines:\n", "    print(f\"\\nHeadline: {headline}\")\n", "    API_RESPONSE = get_completion(\n", "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n", "        model=\"gpt-4\",\n", "    )\n", "    print(f\"Category: {API_RESPONSE.choices[0].message.content}\\n\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¯ä¸ªæ ‡é¢˜çš„é€‰å®šç±»åˆ«ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ— æ³•çœ‹åˆ°æ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ç½®ä¿¡åº¦ã€‚è®©æˆ‘ä»¬é‡æ–°è¿è¡Œç›¸åŒçš„æç¤ºï¼Œä½†å¯ç”¨`logprobs`ï¼Œå¹¶å°†`top_logprobs`è®¾ç½®ä¸º2ï¼ˆè¿™å°†æ˜¾ç¤ºæ¯ä¸ªæ ‡è®°çš„2ä¸ªæœ€å¯èƒ½çš„è¾“å‡ºæ ‡è®°ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¾“å‡ºæ¯ä¸ªè¾“å‡ºæ ‡è®°çš„çº¿æ€§æ¦‚ç‡ï¼Œä»¥ä¾¿å°†å¯¹æ•°æ¦‚ç‡è½¬æ¢ä¸ºæ›´å®¹æ˜“è§£é‡Šçš„0-100%çš„æ¯”ä¾‹ã€‚\n"]}, {"cell_type": "code", "execution_count": 269, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Headline: Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n"]}, {"data": {"text/html": ["<span style='color: cyan'>Output token 1:</span> Technology, <span style='color: darkorange'>logprobs:</span> -2.4584822e-06, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span> Techn, <span style='color: darkorange'>logprobs:</span> -13.781253, <span style='color: magenta'>linear probability:</span> 0.0%<br>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "\n", "Headline: Local Mayor Launches Initiative to Enhance Urban Public Transport.\n"]}, {"data": {"text/html": ["<span style='color: cyan'>Output token 1:</span> Politics, <span style='color: darkorange'>logprobs:</span> -2.4584822e-06, <span style='color: magenta'>linear probability:</span> 100.0%<br><span style='color: cyan'>Output token 2:</span> Technology, <span style='color: darkorange'>logprobs:</span> -13.937503, <span style='color: magenta'>linear probability:</span> 0.0%<br>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "\n", "Headline: Tennis Champion Showcases Hidden Talents in Symphony Orchestra Debut\n"]}, {"data": {"text/html": ["<span style='color: cyan'>Output token 1:</span> Art, <span style='color: darkorange'>logprobs:</span> -0.009169078, <span style='color: magenta'>linear probability:</span> 99.09%<br><span style='color: cyan'>Output token 2:</span> Sports, <span style='color: darkorange'>logprobs:</span> -4.696669, <span style='color: magenta'>linear probability:</span> 0.91%<br>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["\n", "\n"]}], "source": ["for headline in headlines:\n", "    print(f\"\\nHeadline: {headline}\")\n", "    API_RESPONSE = get_completion(\n", "        [{\"role\": \"user\", \"content\": CLASSIFICATION_PROMPT.format(headline=headline)}],\n", "        model=\"gpt-4\",\n", "        logprobs=True,\n", "        top_logprobs=2,\n", "    )\n", "    top_two_logprobs = API_RESPONSE.choices[0].logprobs.content[0].top_logprobs\n", "    html_content = \"\"\n", "    for i, logprob in enumerate(top_two_logprobs, start=1):\n", "        html_content += (\n", "            f\"<span style='color: cyan'>Output token {i}:</span> {logprob.token}, \"\n", "            f\"<span style='color: darkorange'>logprobs:</span> {logprob.logprob}, \"\n", "            f\"<span style='color: magenta'>linear probability:</span> {np.round(np.exp(logprob.logprob)*100,2)}%<br>\"\n", "        )\n", "    display(HTML(html_content))\n", "    print(\"\\n\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["æ­£å¦‚ä»å‰ä¸¤ä¸ªæ ‡é¢˜æ‰€é¢„æœŸçš„é‚£æ ·ï¼Œ`gpt-4` å¯¹å…¶åˆ†ç±»å‡ ä¹æœ‰100%çš„ä¿¡å¿ƒï¼Œå› ä¸ºå†…å®¹åˆ†åˆ«æ˜æ˜¾èšç„¦åœ¨æŠ€æœ¯å’Œæ”¿æ²»é¢†åŸŸã€‚ç„¶è€Œï¼Œç¬¬ä¸‰ä¸ªæ ‡é¢˜ç»“åˆäº†ä½“è‚²å’Œä¸è‰ºæœ¯ç›¸å…³çš„ä¸»é¢˜ï¼Œå› æ­¤æˆ‘ä»¬çœ‹åˆ°æ¨¡å‹å¯¹å…¶é€‰æ‹©çš„ä¿¡å¿ƒè¾ƒä½ã€‚\n", "\n", "è¿™æ˜¾ç¤ºäº†ä½¿ç”¨`logprobs`çš„é‡è¦æ€§ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬å°†LLMsç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ç½®ä¿¡é˜ˆå€¼ï¼Œæˆ–è€…å¦‚æœæ‰€é€‰è¾“å‡ºçš„å¯¹æ•°æ¦‚ç‡ä¸å¤Ÿé«˜ï¼Œå¯ä»¥è¾“å‡ºå‡ ä¸ªæ½œåœ¨çš„è¾“å‡ºæ ‡è®°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨åˆ›å»ºä¸€ä¸ªç”¨äºæ ‡è®°æ–‡ç« çš„æ¨èå¼•æ“ï¼Œæˆ‘ä»¬å¯ä»¥è‡ªåŠ¨åˆ†ç±»è·¨è¶Šä¸€å®šé˜ˆå€¼çš„æ ‡é¢˜ï¼Œå¹¶å°†ä¸å¤Ÿç¡®å®šçš„æ ‡é¢˜å‘é€è¿›è¡Œæ‰‹åŠ¨å®¡æ ¸ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. æ£€ç´¢ç½®ä¿¡åº¦è¯„åˆ†ä»¥å‡å°‘å¹»è§‰\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ä¸ºäº†å‡å°‘å¹»è§‰ï¼Œå¹¶æé«˜åŸºäºRAGçš„é—®ç­”ç³»ç»Ÿçš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`logprobs`æ¥è¯„ä¼°æ¨¡å‹åœ¨æ£€ç´¢æ–¹é¢çš„è‡ªä¿¡ç¨‹åº¦ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å‡è®¾æˆ‘ä»¬å·²ç»ä½¿ç”¨RAGæ„å»ºäº†ä¸€ä¸ªç”¨äºé—®ç­”çš„æ£€ç´¢ç³»ç»Ÿï¼Œä½†æ˜¯æˆ‘ä»¬åœ¨å›ç­”é—®é¢˜æ—¶é‡åˆ°äº†è™šæ„çš„ç­”æ¡ˆå›°éš¾ã€‚*æ³¨æ„ï¼š*æˆ‘ä»¬å°†åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨ç¡¬ç¼–ç çš„æ–‡ç« ï¼Œä½†æ˜¯è¯·æŸ¥çœ‹é£Ÿè°±ä¸­çš„å…¶ä»–æ¡ç›®ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨RAGè¿›è¡Œé—®ç­”çš„æ•™ç¨‹ã€‚\n"]}, {"cell_type": "code", "execution_count": 270, "metadata": {}, "outputs": [], "source": ["# æ–‡ç« è·å–\n", "ada_lovelace_article = \"\"\"Augusta Ada King, Countess of Lovelace (nÃ©e Byron; 10 December 1815 â€“ 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation.\n", "Ada Byron was the only legitimate child of poet Lord Byron and reformer Lady Byron. All Lovelace's half-siblings, Lord Byron's other children, were born out of wedlock to other women. Byron separated from his wife a month after Ada was born and left England forever. He died in Greece when Ada was eight. Her mother was anxious about her upbringing and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her father's perceived insanity. Despite this, Ada remained interested in him, naming her two sons Byron and Gordon. Upon her death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess of Lovelace.\n", "Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday, and the author Charles Dickens, contacts which she used to further her education. Ada described her approach as \"poetical science\" and herself as an \"Analyst (& Metaphysician)\".\n", "When she was eighteen, her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage, who is known as \"the father of computers\". She was in particular interested in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville.\n", "Between 1842 and 1843, Ada translated an article by the military engineer Luigi Menabrea (later Prime Minister of Italy) about the Analytical Engine, supplementing it with an elaborate set of seven notes, simply called \"Notes\".\n", "Lovelace's notes are important in the early history of computers, especially since the seventh one contained what many consider to be the first computer programâ€”that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 contain the first programs for the engine. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mindset of \"poetical science\" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.\n", "\"\"\"\n", "\n", "# æ ¹æ®æ–‡ç« å†…å®¹å¯ä»¥è½»æ¾å›ç­”çš„é—®é¢˜\n", "easy_questions = [\n", "    \"What nationality was Ada Lovelace?\",\n", "    \"What was an important finding from Lovelace's seventh note?\",\n", "]\n", "\n", "# æ–‡ç« ä¸­æœªå®Œå…¨æ¶µç›–çš„é—®é¢˜\n", "medium_questions = [\n", "    \"Did Lovelace collaborate with Charles Dickens\",\n", "    \"What concepts did Lovelace build with Charles Babbage\",\n", "]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¦æ±‚æ¨¡å‹å›ç­”é—®é¢˜ï¼Œç„¶åè¯„ä¼°å…¶å›ç­”ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è¦æ±‚æ¨¡å‹è¾“å‡ºä¸€ä¸ªå¸ƒå°”å€¼ `has_sufficient_context_for_answer`ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è¯„ä¼° `logprobs` æ¥æŸ¥çœ‹æ¨¡å‹å¯¹å…¶å›ç­”æ˜¯å¦åŒ…å«åœ¨æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æœ‰å¤šè‡ªä¿¡ã€‚\n"]}, {"cell_type": "code", "execution_count": 271, "metadata": {}, "outputs": [], "source": ["PROMPT = \"\"\"You retrieved this article: {article}. The question is: {question}.\n", "Before even answering the question, consider whether you have sufficient information in the article to answer the question fully.\n", "Your output should JUST be the boolean true or false, of if you have sufficient information in the article to answer the question.\n", "Respond with just one word, the boolean true or false. You must output the word 'True', or the word 'False', nothing else.\n", "\"\"\"\n", "\n"]}, {"cell_type": "code", "execution_count": 272, "metadata": {}, "outputs": [{"data": {"text/html": ["Questions clearly answered in article<p style=\"color:green\">Question: What nationality was Ada Lovelace?</p><p style=\"color:cyan\">has_sufficient_context_for_answer: True, <span style=\"color:darkorange\">logprobs: -3.1281633e-07, <span style=\"color:magenta\">linear probability: 100.0%</span></p><p style=\"color:green\">Question: What was an important finding from Lovelace's seventh note?</p><p style=\"color:cyan\">has_sufficient_context_for_answer: True, <span style=\"color:darkorange\">logprobs: -7.89631e-07, <span style=\"color:magenta\">linear probability: 100.0%</span></p>Questions only partially covered in the article<p style=\"color:green\">Question: Did Lovelace collaborate with Charles Dickens</p><p style=\"color:cyan\">has_sufficient_context_for_answer: True, <span style=\"color:darkorange\">logprobs: -0.06993677, <span style=\"color:magenta\">linear probability: 93.25%</span></p><p style=\"color:green\">Question: What concepts did Lovelace build with Charles Babbage</p><p style=\"color:cyan\">has_sufficient_context_for_answer: False, <span style=\"color:darkorange\">logprobs: -0.61807257, <span style=\"color:magenta\">linear probability: 53.9%</span></p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["html_output = \"\"\n", "html_output += \"Questions clearly answered in article\"\n", "\n", "for question in easy_questions:\n", "    API_RESPONSE = get_completion(\n", "        [\n", "            {\n", "                \"role\": \"user\",\n", "                \"content\": PROMPT.format(\n", "                    article=ada_lovelace_article, question=question\n", "                ),\n", "            }\n", "        ],\n", "        model=\"gpt-4\",\n", "        logprobs=True,\n", "    )\n", "    html_output += f'<p style=\"color:green\">Question: {question}</p>'\n", "    for logprob in API_RESPONSE.choices[0].logprobs.content:\n", "        html_output += f'<p style=\"color:cyan\">has_sufficient_context_for_answer: {logprob.token}, <span style=\"color:darkorange\">logprobs: {logprob.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(logprob.logprob)*100,2)}%</span></p>'\n", "\n", "html_output += \"Questions only partially covered in the article\"\n", "\n", "for question in medium_questions:\n", "    API_RESPONSE = get_completion(\n", "        [\n", "            {\n", "                \"role\": \"user\",\n", "                \"content\": PROMPT.format(\n", "                    article=ada_lovelace_article, question=question\n", "                ),\n", "            }\n", "        ],\n", "        model=\"gpt-4\",\n", "        logprobs=True,\n", "        top_logprobs=3,\n", "    )\n", "    html_output += f'<p style=\"color:green\">Question: {question}</p>'\n", "    for logprob in API_RESPONSE.choices[0].logprobs.content:\n", "        html_output += f'<p style=\"color:cyan\">has_sufficient_context_for_answer: {logprob.token}, <span style=\"color:darkorange\">logprobs: {logprob.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(logprob.logprob)*100,2)}%</span></p>'\n", "\n", "display(HTML(html_output))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å¯¹äºå‰ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ–­è¨€ï¼ˆå‡ ä¹ï¼‰æœ‰100%çš„ä¿¡å¿ƒè®¤ä¸ºæ–‡ç« å…·æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ¥å›ç­”æå‡ºçš„é—®é¢˜ã€‚\n", "\n", "å¦ä¸€æ–¹é¢ï¼Œå¯¹äºé‚£äº›åœ¨æ–‡ç« ä¸­å›ç­”ä¸å¤ªæ˜ç¡®çš„æ›´æ£˜æ‰‹çš„é—®é¢˜ï¼Œæ¨¡å‹å¯¹è‡ªå·±æ˜¯å¦å…·æœ‰è¶³å¤Ÿä¸Šä¸‹æ–‡çš„ä¿¡å¿ƒè¾ƒä½ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é˜²æŠ¤æªæ–½ï¼Œæœ‰åŠ©äºç¡®ä¿æˆ‘ä»¬æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯è¶³å¤Ÿçš„ã€‚\n", "\n", "è¿™ç§è‡ªæˆ‘è¯„ä¼°å¯ä»¥å¸®åŠ©å‡å°‘å¹»è§‰ï¼Œå› ä¸ºå½“æ‚¨çš„`sufficient_context_for_answer`å¯¹æ•°æ¦‚ç‡ä½äºä¸€å®šé˜ˆå€¼æ—¶ï¼Œæ‚¨å¯ä»¥é™åˆ¶ç­”æ¡ˆæˆ–é‡æ–°æç¤ºç”¨æˆ·ã€‚å·²ç»è¯æ˜ï¼Œè¿™æ ·çš„æ–¹æ³•å¯ä»¥æ˜¾è‘—å‡å°‘é—®ç­”å¹»è§‰å’Œé”™è¯¯çš„å‘ç”Ÿç‡ï¼ˆ[ç¤ºä¾‹](https://jfan001.medium.com/how-we-cut-the-rate-of-gpt-hallucinations-from-20-to-less-than-2-f3bfcc10e4ec)ï¼‰ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. è‡ªåŠ¨å®ŒæˆåŠŸèƒ½\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["`logprobs` çš„å¦ä¸€ä¸ªç”¨ä¾‹æ˜¯è‡ªåŠ¨è¡¥å…¨ç³»ç»Ÿã€‚åœ¨ä¸éœ€è¦ä»å¤´åˆ°å°¾åˆ›å»ºæ•´ä¸ªè‡ªåŠ¨è¡¥å…¨ç³»ç»Ÿçš„æƒ…å†µä¸‹ï¼Œè®©æˆ‘ä»¬æ¼”ç¤ºä¸€ä¸‹å¦‚ä½•åˆ©ç”¨ `logprobs` æ¥å¸®åŠ©æˆ‘ä»¬å†³å®šåœ¨ç”¨æˆ·è¾“å…¥æ—¶å¦‚ä½•å»ºè®®å•è¯ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ„é€ ä¸€ä¸ªç¤ºä¾‹å¥å­ï¼š`\"æˆ‘æœ€ä¸å–œæ¬¢çš„ç”µè§†èŠ‚ç›®æ˜¯ç»å‘½æ¯’å¸ˆã€‚\"` å‡è®¾æˆ‘ä»¬å¸Œæœ›åœ¨æˆ‘ä»¬è¾“å…¥å¥å­æ—¶åŠ¨æ€æ¨èä¸‹ä¸€ä¸ªå•è¯æˆ–æ ‡è®°ï¼Œä½†*ä»…å½“*æ¨¡å‹éå¸¸ç¡®å®šä¸‹ä¸€ä¸ªå•è¯æ˜¯ä»€ä¹ˆæ—¶ã€‚ä¸ºäº†æ¼”ç¤ºè¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬å°†å¥å­åˆ†è§£ä¸ºé¡ºåºç»„ä»¶ã€‚\n"]}, {"cell_type": "code", "execution_count": 273, "metadata": {}, "outputs": [], "source": ["sentence_list = [\n", "    \"My\",\n", "    \"My least\",\n", "    \"My least favorite\",\n", "    \"My least favorite TV\",\n", "    \"My least favorite TV show\",\n", "    \"My least favorite TV show is\",\n", "    \"My least favorite TV show is Breaking Bad\",\n", "]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¦æ±‚`gpt-3.5-turbo`å……å½“ä¸€ä¸ªè‡ªåŠ¨è¡¥å…¨å¼•æ“ï¼Œä½¿ç”¨æ¨¡å‹æ‰€ç»™å®šçš„ä»»ä½•ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬å¯ä»¥å¯ç”¨`logprobs`ï¼Œå¹¶æŸ¥çœ‹æ¨¡å‹å¯¹å…¶é¢„æµ‹çš„è‡ªä¿¡ç¨‹åº¦ã€‚\n"]}, {"cell_type": "code", "execution_count": 274, "metadata": {}, "outputs": [{"data": {"text/html": ["<p>Sentence: My</p><p style=\"color:cyan\">Predicted next token: favorite, <span style=\"color:darkorange\">logprobs: -0.18245785, <span style=\"color:magenta\">linear probability: 83.32%</span></p><p style=\"color:cyan\">Predicted next token: dog, <span style=\"color:darkorange\">logprobs: -2.397172, <span style=\"color:magenta\">linear probability: 9.1%</span></p><p style=\"color:cyan\">Predicted next token: ap, <span style=\"color:darkorange\">logprobs: -3.8732424, <span style=\"color:magenta\">linear probability: 2.08%</span></p><br><p>Sentence: My least</p><p style=\"color:cyan\">Predicted next token: favorite, <span style=\"color:darkorange\">logprobs: -0.0146376295, <span style=\"color:magenta\">linear probability: 98.55%</span></p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -4.2417912, <span style=\"color:magenta\">linear probability: 1.44%</span></p><p style=\"color:cyan\">Predicted next token:  favorite, <span style=\"color:darkorange\">logprobs: -9.748788, <span style=\"color:magenta\">linear probability: 0.01%</span></p><br><p>Sentence: My least favorite</p><p style=\"color:cyan\">Predicted next token: food, <span style=\"color:darkorange\">logprobs: -0.9481721, <span style=\"color:magenta\">linear probability: 38.74%</span></p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -1.3447137, <span style=\"color:magenta\">linear probability: 26.06%</span></p><p style=\"color:cyan\">Predicted next token: color, <span style=\"color:darkorange\">logprobs: -1.3887696, <span style=\"color:magenta\">linear probability: 24.94%</span></p><br><p>Sentence: My least favorite TV</p><p style=\"color:cyan\">Predicted next token: show, <span style=\"color:darkorange\">logprobs: -0.0007898556, <span style=\"color:magenta\">linear probability: 99.92%</span></p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -7.711523, <span style=\"color:magenta\">linear probability: 0.04%</span></p><p style=\"color:cyan\">Predicted next token: series, <span style=\"color:darkorange\">logprobs: -9.348547, <span style=\"color:magenta\">linear probability: 0.01%</span></p><br><p>Sentence: My least favorite TV show</p><p style=\"color:cyan\">Predicted next token: is, <span style=\"color:darkorange\">logprobs: -0.2851253, <span style=\"color:magenta\">linear probability: 75.19%</span></p><p style=\"color:cyan\">Predicted next token: of, <span style=\"color:darkorange\">logprobs: -1.55335, <span style=\"color:magenta\">linear probability: 21.15%</span></p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -3.4928775, <span style=\"color:magenta\">linear probability: 3.04%</span></p><br><p>Sentence: My least favorite TV show is</p><p style=\"color:cyan\">Predicted next token: \"My, <span style=\"color:darkorange\">logprobs: -0.69349754, <span style=\"color:magenta\">linear probability: 49.98%</span></p><p style=\"color:cyan\">Predicted next token: \"The, <span style=\"color:darkorange\">logprobs: -1.2899293, <span style=\"color:magenta\">linear probability: 27.53%</span></p><p style=\"color:cyan\">Predicted next token: My, <span style=\"color:darkorange\">logprobs: -2.4170141, <span style=\"color:magenta\">linear probability: 8.92%</span></p><br><p>Sentence: My least favorite TV show is Breaking Bad</p><p style=\"color:cyan\">Predicted next token: because, <span style=\"color:darkorange\">logprobs: -0.17786823, <span style=\"color:magenta\">linear probability: 83.71%</span></p><p style=\"color:cyan\">Predicted next token: ,, <span style=\"color:darkorange\">logprobs: -2.3946173, <span style=\"color:magenta\">linear probability: 9.12%</span></p><p style=\"color:cyan\">Predicted next token: ., <span style=\"color:darkorange\">logprobs: -3.1861975, <span style=\"color:magenta\">linear probability: 4.13%</span></p><br>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["high_prob_completions = {}\n", "low_prob_completions = {}\n", "html_output = \"\"\n", "\n", "for sentence in sentence_list:\n", "    PROMPT = \"\"\"å®Œæˆè¿™ä¸ªå¥å­ã€‚ä½ æ­£åœ¨æ‰®æ¼”è‡ªåŠ¨è¡¥å…¨çš„è§’è‰²ã€‚åªéœ€å°½ä½ æ‰€èƒ½åœ°å®Œæˆè¿™ä¸ªå¥å­ï¼Œç¡®ä¿å®ƒåªæ˜¯ä¸€å¥è¯ï¼š{sentence}\"\"\"\n", "    API_RESPONSE = get_completion(\n", "        [{\"role\": \"user\", \"content\": PROMPT.format(sentence=sentence)}],\n", "        model=\"gpt-3.5-turbo\",\n", "        logprobs=True,\n", "        top_logprobs=3,\n", "    )\n", "    html_output += f'<p>Sentence: {sentence}</p>'\n", "    first_token = True\n", "    for token in API_RESPONSE.choices[0].logprobs.content[0].top_logprobs:\n", "        html_output += f'<p style=\"color:cyan\">Predicted next token: {token.token}, <span style=\"color:darkorange\">logprobs: {token.logprob}, <span style=\"color:magenta\">linear probability: {np.round(np.exp(token.logprob)*100,2)}%</span></p>'\n", "        if first_token:\n", "            if np.exp(token.logprob) > 0.95:\n", "                high_prob_completions[sentence] = token.token\n", "            if np.exp(token.logprob) < 0.60:\n", "                low_prob_completions[sentence] = token.token\n", "        first_token = False\n", "    html_output += \"<br>\"\n", "\n", "display(HTML(html_output))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹é«˜ç½®ä¿¡åº¦çš„è‡ªåŠ¨è¡¥å…¨ï¼š\n"]}, {"cell_type": "code", "execution_count": 275, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'My least': 'favorite', 'My least favorite TV': 'show'}"]}, "execution_count": 275, "metadata": {}, "output_type": "execute_result"}], "source": ["high_prob_completions\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["è¿™äº›çœ‹èµ·æ¥å¾ˆåˆç†ï¼æˆ‘ä»¬å¯ä»¥å¯¹è¿™äº›å»ºè®®æ„Ÿåˆ°è‡ªä¿¡ã€‚åœ¨å†™å®Œ'My least favorite TV'åï¼Œå¾ˆå¯èƒ½ä½ æƒ³å†™'show'ï¼ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹å¯¹è‡ªåŠ¨å®Œæˆå»ºè®®ä¸å¤ªæœ‰ä¿¡å¿ƒçš„éƒ¨åˆ†ï¼š\n"]}, {"cell_type": "code", "execution_count": 276, "metadata": {}, "outputs": [{"data": {"text/plain": ["{'My least favorite': 'food', 'My least favorite TV show is': '\"My'}"]}, "execution_count": 276, "metadata": {}, "output_type": "execute_result"}], "source": ["low_prob_completions\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["è¿™äº›ä¹Ÿæ˜¯é€»è¾‘çš„ã€‚ä»…å‡­å‰ç¼€â€œæˆ‘æœ€ä¸å–œæ¬¢â€ï¼Œç”¨æˆ·è¦è¡¨è¾¾ä»€ä¹ˆå¹¶ä¸æ¸…æ¥šï¼Œä½œè€…æœ€å–œæ¬¢çš„ç”µè§†èŠ‚ç›®æ˜¯ä»€ä¹ˆä¹Ÿåªèƒ½ç”±ä»»ä½•äººçŒœæµ‹ã€‚<br><br>\n", "å› æ­¤ï¼Œä½¿ç”¨`gpt-3.5-turbo`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`logprobs`åˆ›å»ºä¸€ä¸ªåŠ¨æ€è‡ªåŠ¨å®Œæˆå¼•æ“çš„æ ¹ï¼\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. é«˜äº®å™¨å’Œå­—èŠ‚å‚æ•°\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["è®©æˆ‘ä»¬å¿«é€Ÿäº†è§£å¦‚ä½•ä½¿ç”¨`logprobs`å’Œ`bytes`å‚æ•°åˆ›å»ºä¸€ä¸ªç®€å•çš„æ ‡è®°é«˜äº®å™¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å¹¶çªå‡ºæ˜¾ç¤ºæ¯ä¸ªæ ‡è®°ã€‚è™½ç„¶è¿™ä¸ä½¿ç”¨å¯¹æ•°æ¦‚ç‡ï¼Œä½†å®ƒä½¿ç”¨äº†å¯ç”¨`logprobs`æ—¶é™„å¸¦çš„å†…ç½®æ ‡è®°åŒ–åŠŸèƒ½ã€‚\n"]}, {"cell_type": "code", "execution_count": 277, "metadata": {}, "outputs": [], "source": ["PROMPT = \"\"\"è‹±è¯­ä¸­æœ€é•¿çš„å•è¯æ˜¯ä»€ä¹ˆï¼Ÿ\"\"\"\n", "\n", "API_RESPONSE = get_completion(\n", "    [{\"role\": \"user\", \"content\": PROMPT}], model=\"gpt-4\", logprobs=True, top_logprobs=5\n", ")\n", "\n", "\n", "def highlight_text(api_response):\n", "    colors = [\n", "        \"#FF00FF\",  # Magenta\n", "        \"#008000\",  # Green\n", "        \"#FF8C00\",  # Dark Orange\n", "        \"#FF0000\",  # Red\n", "        \"#0000FF\",  # Blue\n", "    ]\n", "    tokens = api_response.choices[0].logprobs.content\n", "\n", "    color_idx = 0  # Initialize color index\n", "    html_output = \"\"  # Initialize HTML output\n", "    for t in tokens:\n", "        token_str = bytes(t.bytes).decode(\"utf-8\")  # Decode bytes to string\n", "\n", "        # Add colored token to HTML output\n", "        html_output += f\"<span style='color: {colors[color_idx]}'>{token_str}</span>\"\n", "\n", "        # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé¢œè‰²\n", "        color_idx = (color_idx + 1) % len(colors)\n", "    display(HTML(html_output))  # æ˜¾ç¤º HTML è¾“å‡º\n", "    print(f\"Total number of tokens: {len(tokens)}\")\n"]}, {"cell_type": "code", "execution_count": 278, "metadata": {}, "outputs": [{"data": {"text/html": ["<span style='color: #FF00FF'>The</span><span style='color: #008000'> longest</span><span style='color: #FF8C00'> word</span><span style='color: #FF0000'> in</span><span style='color: #0000FF'> the</span><span style='color: #FF00FF'> English</span><span style='color: #008000'> language</span><span style='color: #FF8C00'>,</span><span style='color: #FF0000'> according</span><span style='color: #0000FF'> to</span><span style='color: #FF00FF'> the</span><span style='color: #008000'> Guinness</span><span style='color: #FF8C00'> World</span><span style='color: #FF0000'> Records</span><span style='color: #0000FF'>,</span><span style='color: #FF00FF'> is</span><span style='color: #008000'> '</span><span style='color: #FF8C00'>p</span><span style='color: #FF0000'>ne</span><span style='color: #0000FF'>um</span><span style='color: #FF00FF'>on</span><span style='color: #008000'>oul</span><span style='color: #FF8C00'>tram</span><span style='color: #FF0000'>icro</span><span style='color: #0000FF'>sc</span><span style='color: #FF00FF'>op</span><span style='color: #008000'>ics</span><span style='color: #FF8C00'>il</span><span style='color: #FF0000'>ic</span><span style='color: #0000FF'>ov</span><span style='color: #FF00FF'>ol</span><span style='color: #008000'>cano</span><span style='color: #FF8C00'>con</span><span style='color: #FF0000'>iosis</span><span style='color: #0000FF'>'.</span><span style='color: #FF00FF'> It</span><span style='color: #008000'> is</span><span style='color: #FF8C00'> a</span><span style='color: #FF0000'> type</span><span style='color: #0000FF'> of</span><span style='color: #FF00FF'> lung</span><span style='color: #008000'> disease</span><span style='color: #FF8C00'> caused</span><span style='color: #FF0000'> by</span><span style='color: #0000FF'> inh</span><span style='color: #FF00FF'>aling</span><span style='color: #008000'> ash</span><span style='color: #FF8C00'> and</span><span style='color: #FF0000'> sand</span><span style='color: #0000FF'> dust</span><span style='color: #FF00FF'>.</span>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Total number of tokens: 51\n"]}], "source": ["highlight_text(API_RESPONSE)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨byteså‚æ•°é‡æ„ä¸€ä¸ªå¥å­ã€‚å¯ç”¨`logprobs`åï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°æ¯ä¸ªæ ‡è®°ä»¥åŠè¯¥æ ‡è®°å­—ç¬¦ä¸²çš„ASCIIï¼ˆåè¿›åˆ¶utf-8ï¼‰å€¼ã€‚å¤„ç†åŒ…å«è¡¨æƒ…ç¬¦å·æˆ–ç‰¹æ®Šå­—ç¬¦çš„æ ‡è®°æ—¶ï¼Œè¿™äº›ASCIIå€¼å¯èƒ½ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚\n"]}, {"cell_type": "code", "execution_count": 279, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Token: \\xf0\\x9f\\x92\n", "Log prob: -0.0003056686\n", "Linear prob: 99.97 %\n", "Bytes: [240, 159, 146] \n", "\n", "Token: \\x99\n", "Log prob: 0.0\n", "Linear prob: 100.0 %\n", "Bytes: [153] \n", "\n", "Token:  -\n", "Log prob: -0.0096905725\n", "Linear prob: 99.04 %\n", "Bytes: [32, 45] \n", "\n", "Token:  Blue\n", "Log prob: -0.00042042506\n", "Linear prob: 99.96 %\n", "Bytes: [32, 66, 108, 117, 101] \n", "\n", "Token:  Heart\n", "Log prob: -7.302705e-05\n", "Linear prob: 99.99 %\n", "Bytes: [32, 72, 101, 97, 114, 116] \n", "\n", "Bytes array: [240, 159, 146, 153, 32, 45, 32, 66, 108, 117, 101, 32, 72, 101, 97, 114, 116]\n", "Decoded bytes: ğŸ’™ - Blue Heart\n", "Joint prob: 98.96 %\n"]}], "source": ["PROMPT = \"\"\"è¾“å‡ºè“è‰²å¿ƒå½¢è¡¨æƒ…ç¬¦å·åŠå…¶åç§°ã€‚\"\"\"\n", "API_RESPONSE = get_completion(\n", "    [{\"role\": \"user\", \"content\": PROMPT}], model=\"gpt-4\", logprobs=True\n", ")\n", "\n", "aggregated_bytes = []\n", "joint_logprob = 0.0\n", "\n", "# éå†å„ä¸ªè¯å…ƒï¼Œèšåˆå­—èŠ‚å¹¶è®¡ç®—è”åˆå¯¹æ•°æ¦‚ç‡\n", "for token in API_RESPONSE.choices[0].logprobs.content:\n", "    print(\"Token:\", token.token)\n", "    print(\"Log prob:\", token.logprob)\n", "    print(\"Linear prob:\", np.round(exp(token.logprob) * 100, 2), \"%\")\n", "    print(\"Bytes:\", token.bytes, \"\\n\")\n", "    aggregated_bytes += token.bytes\n", "    joint_logprob += token.logprob\n", "\n", "# å°†èšåˆçš„å­—èŠ‚è§£ç ä¸ºæ–‡æœ¬\n", "aggregated_text = bytes(aggregated_bytes).decode(\"utf-8\")\n", "\n", "# æ–­è¨€è§£ç åçš„æ–‡æœ¬ä¸æ¶ˆæ¯å†…å®¹ç›¸åŒ\n", "assert API_RESPONSE.choices[0].message.content == aggregated_text\n", "\n", "# æ‰“å°ç»“æœ\n", "print(\"Bytes array:\", aggregated_bytes)\n", "print(f\"Decoded bytes: {aggregated_text}\")\n", "print(\"Joint prob:\", np.round(exp(joint_logprob) * 100, 2), \"%\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°ç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯`\\xf0\\x9f\\x92'`ï¼Œæˆ‘ä»¬å¯ä»¥è·å–å®ƒçš„ASCIIå€¼å¹¶å°†å…¶é™„åŠ åˆ°ä¸€ä¸ªå­—èŠ‚æ•°ç»„ä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°†è¿™ä¸ªæ•°ç»„è§£ç ä¸ºä¸€ä¸ªå®Œæ•´çš„å¥å­ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬çš„æ–­è¨€è¯­å¥éªŒè¯è§£ç åçš„å­—èŠ‚ä¸æˆ‘ä»¬çš„å®Œæˆæ¶ˆæ¯ç›¸åŒï¼\n", "\n", "æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ•´ä¸ªå®Œæˆçš„è”åˆæ¦‚ç‡ï¼Œè¿™æ˜¯æ¯ä¸ªæ ‡è®°å¯¹æ•°æ¦‚ç‡çš„æŒ‡æ•°ä¹˜ç§¯ã€‚è¿™ç»™å‡ºäº†åœ¨ç»™å®šæç¤ºçš„æƒ…å†µä¸‹è¿™ä¸ªå®Œæˆæœ‰å¤šä¹ˆ`å¯èƒ½`ã€‚ç”±äºæˆ‘ä»¬çš„æç¤ºç›¸å½“æ˜ç¡®ï¼ˆè¦æ±‚ç‰¹å®šçš„è¡¨æƒ…ç¬¦å·åŠå…¶åç§°ï¼‰ï¼Œè¿™ä¸ªè¾“å‡ºçš„è”åˆæ¦‚ç‡å¾ˆé«˜ï¼ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬è¦æ±‚ä¸€ä¸ªéšæœºè¾“å‡ºï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ä¸€ä¸ªæ›´ä½çš„è”åˆæ¦‚ç‡ã€‚è¿™ä¹Ÿå¯ä»¥æ˜¯å¼€å‘äººå‘˜åœ¨æç¤ºå·¥ç¨‹ä¸­çš„ä¸€ä¸ªå¥½ç­–ç•¥ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. è®¡ç®—å›°æƒ‘åº¦\n", "\n", "åœ¨è¯„ä¼°æ¨¡å‹å¯¹ç»“æœçš„ä¿¡å¿ƒæ—¶ï¼Œè®¡ç®—å›°æƒ‘åº¦å¯èƒ½æ˜¯æœ‰ç”¨çš„ï¼Œå®ƒæ˜¯ä¸€ç§è¡¡é‡ä¸ç¡®å®šæ€§çš„æŒ‡æ ‡ã€‚å›°æƒ‘åº¦å¯ä»¥é€šè¿‡å¯¹logprobsçš„å¹³å‡å€¼å–è´ŸæŒ‡æ•°æ¥è®¡ç®—ã€‚é€šå¸¸ï¼Œè¾ƒé«˜çš„å›°æƒ‘åº¦è¡¨ç¤ºç»“æœæ›´ä¸ç¡®å®šï¼Œè€Œè¾ƒä½çš„å›°æƒ‘åº¦è¡¨ç¤ºç»“æœæ›´æœ‰ä¿¡å¿ƒã€‚å› æ­¤ï¼Œå›°æƒ‘åº¦å¯ä»¥ç”¨äºè¯„ä¼°å•ä¸ªæ¨¡å‹è¿è¡Œçš„ç»“æœï¼Œä¹Ÿå¯ä»¥ç”¨äºæ¯”è¾ƒä¸åŒæ¨¡å‹è¿è¡Œä¹‹é—´ç»“æœçš„ç›¸å¯¹ä¿¡å¿ƒæ°´å¹³ã€‚è™½ç„¶é«˜ä¿¡å¿ƒå¹¶ä¸èƒ½ä¿è¯ç»“æœçš„å‡†ç¡®æ€§ï¼Œä½†å®ƒå¯ä»¥ä½œä¸ºä¸€ä¸ªæœ‰ç”¨çš„ä¿¡å·ï¼Œå¯ä»¥ä¸å…¶ä»–è¯„ä¼°æŒ‡æ ‡é…åˆä½¿ç”¨ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ‚¨çš„æç¤ºè¡Œä¸ºã€‚\n", "\n", "ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘æƒ³ä½¿ç”¨`gpt-3.5-turbo`æ¥äº†è§£æ›´å¤šå…³äºäººå·¥æ™ºèƒ½çš„ä¿¡æ¯ã€‚æˆ‘å¯ä»¥æå‡ºä¸€ä¸ªå…³äºæœ€è¿‘å†å²çš„é—®é¢˜å’Œä¸€ä¸ªå…³äºæœªæ¥çš„é—®é¢˜ï¼š\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Prompt:     In a short sentence, has artifical intelligence grown in the last decade?\n", "Response:   Yes, artificial intelligence has grown significantly in the last decade. \n", "\n", "Tokens:                Yes              ,     artificial   intelligence            has          grown  significantly             in            the           last         decade              .\n", "Logprobs:            -0.00          -0.00          -0.00          -0.00          -0.00          -0.53          -0.11          -0.00          -0.00          -0.01          -0.00          -0.00\n", "Perplexity: 1.0564125277713383 \n", "\n", "Prompt:     In a short sentence, what are your thoughts on the future of artificial intelligence?\n", "Response:   The future of artificial intelligence holds great potential for transforming industries and improving efficiency, but also raises ethical and societal concerns that must be carefully addressed. \n", "\n", "Tokens:               The        future            of    artificial  intelligence         holds         great     potential           for  transforming    industries           and     improving    efficiency             ,           but          also        raises       ethical           and      societal      concerns          that          must            be     carefully     addressed             .\n", "Logprobs:           -0.19         -0.03         -0.00         -0.00         -0.00         -0.30         -0.51         -0.24         -0.03         -1.45         -0.23         -0.03         -0.22         -0.83         -0.48         -0.01         -0.38         -0.07         -0.47         -0.63         -0.18         -0.26         -0.01         -0.14         -0.00         -0.59         -0.55         -0.00\n", "Perplexity: 1.3220795252314004 \n", "\n"]}], "source": ["prompts = [\n", "    \"In a short sentence, has artifical intelligence grown in the last decade?\",\n", "    \"In a short sentence, what are your thoughts on the future of artificial intelligence?\",\n", "]\n", "\n", "for prompt in prompts:\n", "    API_RESPONSE = get_completion(\n", "        [{\"role\": \"user\", \"content\": prompt}],\n", "        model=\"gpt-3.5-turbo\",\n", "        logprobs=True,\n", "    )\n", "\n", "    logprobs = [token.logprob for token in API_RESPONSE.choices[0].logprobs.content]\n", "    response_text = API_RESPONSE.choices[0].message.content\n", "    response_text_tokens = [token.token for token in API_RESPONSE.choices[0].logprobs.content]\n", "    max_starter_length = max(len(s) for s in [\"Prompt:\", \"Response:\", \"Tokens:\", \"Logprobs:\", \"Perplexity:\"])\n", "    max_token_length = max(len(s) for s in response_text_tokens)\n", "    \n", "\n", "    formatted_response_tokens = [s.rjust(max_token_length) for s in response_text_tokens]\n", "    formatted_lps = [f\"{lp:.2f}\".rjust(max_token_length) for lp in logprobs]\n", "\n", "    perplexity_score = np.exp(-np.mean(logprobs))\n", "    print(\"Prompt:\".ljust(max_starter_length), prompt)\n", "    print(\"Response:\".ljust(max_starter_length), response_text, \"\\n\")\n", "    print(\"Tokens:\".ljust(max_starter_length), \" \".join(formatted_response_tokens))\n", "    print(\"Logprobs:\".ljust(max_starter_length), \" \".join(formatted_lps))\n", "    print(\"Perplexity:\".ljust(max_starter_length), perplexity_score, \"\\n\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`gpt-3.5-turbo` å¯¹äºå…³äºæœ€è¿‘å†å²çš„æ›´ç¡®å®šæ€§é—®é¢˜è¿”å›äº†ä¸€ä¸ªè¾ƒä½çš„å›°æƒ‘åº¦åˆ†æ•°ï¼Œå¯¹äºå…³äºä¸ä¹…çš„æœªæ¥çš„æ›´æ¨æµ‹æ€§è¯„ä¼°è¿”å›äº†ä¸€ä¸ªè¾ƒé«˜çš„å›°æƒ‘åº¦åˆ†æ•°ã€‚å†æ¬¡å¼ºè°ƒï¼Œè™½ç„¶è¿™äº›å·®å¼‚å¹¶ä¸èƒ½ä¿è¯å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬æœ‰åŠ©äºæŒ‡å¼•æˆ‘ä»¬å¯¹æ¨¡å‹ç»“æœçš„è§£é‡Šä»¥åŠæœªæ¥çš„ä½¿ç”¨æ–¹å¼ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. ç»“è®º\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å¤ªæ£’äº†ï¼æˆ‘ä»¬æˆåŠŸåœ°ä½¿ç”¨äº†`logprobs`å‚æ•°æ¥æ„å»ºä¸€ä¸ªæ›´å¥å£®çš„åˆ†ç±»å™¨ï¼Œè¯„ä¼°æˆ‘ä»¬é—®ç­”ç³»ç»Ÿçš„æ£€ç´¢ï¼Œä»¥åŠå¯¹æˆ‘ä»¬çš„æ ‡è®°çš„æ¯ä¸ªâ€œå­—èŠ‚â€è¿›è¡Œç¼–ç å’Œè§£ç ï¼`logprobs`ä¸ºæˆ‘ä»¬çš„å®Œæˆè¾“å‡ºæ·»åŠ äº†æœ‰ç”¨çš„ä¿¡æ¯å’Œä¿¡å·ï¼Œæˆ‘ä»¬å¾ˆæœŸå¾…çœ‹åˆ°å¼€å‘è€…å¦‚ä½•å°†å…¶æ•´åˆåˆ°åº”ç”¨ç¨‹åºä¸­ä»¥æ”¹è¿›åº”ç”¨ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. å¯èƒ½çš„æ‰©å±•\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["åœ¨è¿™æœ¬é£Ÿè°±ä¸­æ²¡æœ‰æ¶µç›–çš„`logprobs`çš„è®¸å¤šå…¶ä»–ç”¨ä¾‹ã€‚æˆ‘ä»¬å¯ä»¥å°†`logprobs`ç”¨äºï¼š\n", "  - å†…å®¹å®¡æ ¸\n", "  - å…³é”®è¯é€‰æ‹©\n", "  - æ”¹è¿›æç¤ºå’Œè¾“å‡ºçš„å¯è§£é‡Šæ€§\n", "  - ä»¤ç‰Œä¿®å¤\n", "  - ç­‰ç­‰ï¼\n"]}], "metadata": {"kernelspec": {"display_name": "openai", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 4}