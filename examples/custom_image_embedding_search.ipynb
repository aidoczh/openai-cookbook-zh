{"cells": [{"cell_type": "markdown", "metadata": {"id": "w9w5JBaUL-lO"}, "source": ["# 使用CLIP嵌入和GPT-4 Vision的多模态RAG\n"]}, {"cell_type": "markdown", "metadata": {"id": "3CCjcFSiMbvf"}, "source": ["多模态RAG将额外的模态集成到传统的基于文本的RAG中，通过提供额外的上下文和为了改善理解而基于文本数据的基础，增强了LLMs的问答能力。\n", "\n", "采用了来自[服装搭配指南食谱](https://cookbook.openai.com/examples/how_to_combine_gpt4v_with_rag_outfit_assistant)的方法，我们直接嵌入图像进行相似性搜索，绕过了文本字幕的损失过程，以提高检索准确性。\n", "\n", "使用基于CLIP的嵌入进一步允许根据特定数据进行微调或根据未见图像进行更新。\n", "\n", "通过使用用户提供的技术图像搜索企业知识库来展示这一技术，以提供相关信息。\n"]}, {"cell_type": "markdown", "metadata": {"id": "T-Mpdxit4x49"}, "source": ["# 安装说明\n"]}, {"cell_type": "markdown", "metadata": {"id": "nbt3evfHUJTZ"}, "source": ["首先让我们安装相关的包。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "7hgrcVEl0Ma1"}, "outputs": [], "source": ["#安装\n", "%pip install clip\n", "%pip install torch\n", "%pip install pillow\n", "%pip install faiss-cpu\n", "%pip install numpy\n", "%pip install git+https://github.com/openai/CLIP.git\n", "%pip install openai\n"]}, {"cell_type": "markdown", "metadata": {"id": "GgrlBLTpT0si"}, "source": ["然后让我们导入所有需要的包。\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "pN1cWF-iyLUg"}, "outputs": [], "source": ["# 模型导入\n", "import faiss\n", "import json\n", "import torch\n", "from openai import OpenAI\n", "import torch.nn as nn\n", "from torch.utils.data import DataLoader\n", "import clip\n", "client = OpenAI()\n", "\n", "# 辅助导入\n", "from tqdm import tqdm\n", "import json\n", "import os\n", "import numpy as np\n", "import pickle\n", "from typing import List, Union, Tuple\n", "\n", "# 可视化导入\n", "from PIL import Image\n", "import matplotlib.pyplot as plt\n", "import base64\n"]}, {"cell_type": "markdown", "metadata": {"id": "9fONcWxRqll8"}, "source": ["现在让我们加载CLIP模型。\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "_Ua9y98NRk70"}, "outputs": [], "source": ["#在设备上加载模型。您运行推理/训练的设备要么是CPU，要么是GPU（如果您有的话）。\n", "device = \"cpu\"\n", "model, preprocess = clip.load(\"ViT-B/32\",device=device)\n"]}, {"cell_type": "markdown", "metadata": {"id": "Dev-zjfJ774W"}, "source": ["我们现在将：\n", "1. 创建图像嵌入数据库\n", "2. 设置一个查询到视觉模型\n", "3. 执行语义搜索\n", "4. 将用户查询传递给图像\n"]}, {"cell_type": "markdown", "metadata": {"id": "5Y1v2jkS42TS"}, "source": ["# 创建图像嵌入数据库\n"]}, {"cell_type": "markdown", "metadata": {"id": "wVBAMyhesAyi"}, "source": ["接下来，我们将从一个图像目录中创建我们的图像嵌入知识库。这将是我们搜索的技术知识库，用于为用户上传的图像提供信息。\n", "\n", "我们传入存储图像的目录（作为JPEG文件），并循环遍历每个图像以创建我们的嵌入。\n", "\n", "我们还有一个description.json文件。其中包含我们知识库中每个图像的条目。它有两个键：'image_path'和'description'。它将每个图像映射到一个有用的描述，以帮助回答用户的问题。\n"]}, {"cell_type": "markdown", "metadata": {"id": "fDCz76gr8yAu"}, "source": ["首先让我们编写一个函数来获取给定目录中的所有图像路径。然后我们将从名为'image_database'的目录中获取所有的jpeg文件。\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"id": "vE9i3zLuRk5c"}, "outputs": [], "source": ["def get_image_paths(directory: str, number: int = None) -> List[str]:\n", "    image_paths = []\n", "    count = 0\n", "    for filename in os.listdir(directory):\n", "        if filename.endswith('.jpeg'):\n", "            image_paths.append(os.path.join(directory, filename))\n", "            if number is not None and count == number:\n", "                return [image_paths[-1]]\n", "            count += 1\n", "    return image_paths\n", "direc = 'image_database/'\n", "image_paths = get_image_paths(direc)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "hMldfjn189vC"}, "source": ["接下来，我们将编写一个函数，根据一系列路径从CLIP模型中获取图像嵌入。\n", "\n", "我们首先使用之前得到的预处理函数对图像进行预处理。这个函数执行一些操作，以确保输入到CLIP模型的格式和维度正确，包括调整大小、归一化、颜色通道调整等。\n", "\n", "然后，我们将这些预处理后的图像堆叠在一起，这样我们就可以一次将它们传递到模型中，而不是在循环中逐个传递。最后返回模型输出，这是一个嵌入数组。\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"id": "fd3I_fPh8qvi"}, "outputs": [], "source": ["def get_features_from_image_path(image_paths):\n", "  images = [preprocess(Image.open(image_path).convert(\"RGB\")) for image_path in image_paths]\n", "  image_input = torch.tensor(np.stack(images))\n", "  with torch.no_grad():\n", "    image_features = model.encode_image(image_input).float()\n", "  return image_features\n", "image_features = get_features_from_image_path(image_paths)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "UH_kyZAE-kHe"}, "source": ["我们现在可以创建我们的向量数据库。\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"id": "TIeqpndF8tZk"}, "outputs": [], "source": ["index = faiss.IndexFlatIP(image_features.shape[1])\n", "index.add(image_features)\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "swDe1c4v-mbz"}, "source": ["同时摄取我们的json文件，用于图像描述映射，并创建一个json列表。我们还创建一个辅助函数，用于在这个列表中搜索我们想要的图像，以便获取该图像的描述。\n"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"id": "tdjlXQqC8uNE"}, "outputs": [], "source": ["data = []\n", "image_path = 'train1.jpeg'\n", "with open('description.json', 'r') as file:\n", "    for line in file:\n", "        data.append(json.loads(line))\n", "def find_entry(data, key, value):\n", "    for entry in data:\n", "        if entry.get(key) == value:\n", "            return entry\n", "    return None\n"]}, {"cell_type": "markdown", "metadata": {"id": "fJXfCtPD5_63"}, "source": ["让我们展示一个示例图片，这将是用户上传的图片。这是2024年CES上发布的一款科技产品。它是DELTA Pro Ultra全屋电池发电机。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "RtkZ7W3g5sED"}, "outputs": [], "source": ["im = Image.open(image_path)\n", "plt.imshow(im)\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"id": "5ivECCKSdbBy"}, "source": ["![Delta Pro](../images/train1.jpeg)\n"]}, {"cell_type": "markdown", "metadata": {"id": "Sidjylki7Kye"}, "source": ["# 查询视觉模型\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "H8O7X6ml7t38"}, "source": ["现在让我们看看GPT-4 Vision（之前没有见过这项技术）会将其标记为什么。\n"]}, {"cell_type": "markdown", "metadata": {"id": "r4uDjS-gQAqm"}, "source": ["首先，我们需要编写一个函数将我们的图像编码为base64格式，因为这是我们将传递给视觉模型的格式。然后，我们将创建一个通用的image_query函数，允许我们使用图像输入查询LLM。\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 35}, "id": "87gf6_xO8Y4i", "outputId": "99be865f-12e8-4ef0-c2f5-5fd6e5c787f3"}, "outputs": [{"data": {"application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}, "text/plain": ["'Autonomous Delivery Robot'"]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["def encode_image(image_path):\n", "    with open(image_path, 'rb') as image_file:\n", "        encoded_image = base64.b64encode(image_file.read())\n", "        return encoded_image.decode('utf-8')\n", "\n", "def image_query(query, image_path):\n", "    response = client.chat.completions.create(\n", "        model='gpt-4-vision-preview',\n", "        messages=[\n", "            {\n", "            \"role\": \"user\",\n", "            \"content\": [\n", "                {\n", "                \"type\": \"text\",\n", "                \"text\": query,\n", "                },\n", "                {\n", "                \"type\": \"image_url\",\n", "                \"image_url\": {\n", "                    \"url\": f\"data:image/jpeg;base64,{encode_image(image_path)}\",\n", "                },\n", "                }\n", "            ],\n", "            }\n", "        ],\n", "        max_tokens=300,\n", "    )\n", "    # 从响应中提取相关特征\n", "    return response.choices[0].message.content\n", "image_query('Write a short label of what is show in this image?', image_path)\n"]}, {"cell_type": "markdown", "metadata": {"id": "yfG_7c-jQAqm"}, "source": ["正如我们所看到的，模型尽最大努力根据其训练数据进行预测，但由于在训练数据中没有看到类似的内容，因此会出现错误。这是因为这是一幅模糊的图像，使得难以推断和推测。\n"]}, {"cell_type": "markdown", "metadata": {"id": "szWZqTqf7SrA"}, "source": ["# 执行语义搜索\n"]}, {"cell_type": "markdown", "metadata": {"id": "eV8LaOncGH3j"}, "source": ["现在让我们执行相似性搜索，找到我们知识库中最相似的两幅图像。我们通过获取用户输入的图像路径的嵌入，检索数据库中相似图像的索引和距离来实现这一点。距离将是我们的相似性代理指标，距离越小表示相似度越高。然后，我们根据距离按降序排序。\n"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"id": "GzNEhKJ04D-F"}, "outputs": [], "source": ["image_search_embedding = get_features_from_image_path([image_path])\n", "distances, indices = index.search(image_search_embedding.reshape(1, -1), 2) #2 表示要返回的最相似图像的数量。\n", "distances = distances[0]\n", "indices = indices[0]\n", "indices_distances = list(zip(indices, distances))\n", "indices_distances.sort(key=lambda x: x[1], reverse=True)\n"]}, {"cell_type": "markdown", "metadata": {"id": "0O-GYQ-1QAqm"}, "source": ["我们需要这些索引，因为我们将使用它们来搜索我们的图像目录，并选择索引位置处的图像，以供RAG视觉模型使用。\n"]}, {"cell_type": "markdown", "metadata": {"id": "9-6SVzwSJVuT"}, "source": ["让我们看看它带回了什么（我们按相似度顺序显示这些内容）：\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Lt1ZYuKDFeww"}, "outputs": [], "source": ["#显示相似图片\n", "for idx, distance in indices_distances:\n", "    print(idx)\n", "    path = get_image_paths(direc, idx)[0]\n", "    im = Image.open(path)\n", "    plt.imshow(im)\n", "    plt.show()\n"]}, {"cell_type": "markdown", "metadata": {"id": "GPTvKIUJ2tgz"}, "source": ["![Delta Pro2](../images/train2.jpeg)\n", "\n", "![Delta Pro3](../images/train17.jpeg)\n"]}, {"cell_type": "markdown", "metadata": {"id": "x4kF2-MJQAqm"}, "source": ["我们可以看到这里返回了两幅图片，其中包含DELTA Pro Ultra全屋电池发电机。在其中一幅图片中，还有一些可能会分散注意力的背景，但系统成功找到了正确的图片。\n"]}, {"cell_type": "markdown", "metadata": {"id": "Qc2sOKzY7yv3"}, "source": ["# 用户查询最相似的图像\n"]}, {"cell_type": "markdown", "metadata": {"id": "8Sio6OR4MDjI"}, "source": ["现在对于我们最相似的图像，我们希望将其及其描述传递给gpt-v，并附带用户查询，以便他们可以查询他们可能购买的技术。这就是视觉模型的强大之处，您可以提出一般查询，即使模型没有明确接受过相关训练，它也能以高准确度进行回应。\n"]}, {"cell_type": "markdown", "metadata": {"id": "uPzsRk66QAqn"}, "source": ["在下面的示例中，我们将查询所讨论物品的容量。\n"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 87}, "id": "-_5W_xwitbr3", "outputId": "99a40617-0153-492a-d8b0-6782b8421e40"}, "outputs": [{"data": {"application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}, "text/plain": ["'The portable home battery DELTA Pro has a base capacity of 3.6kWh. This capacity can be expanded up to 25kWh with additional batteries. The image showcases the DELTA Pro, which has an impressive 3600W power capacity for AC output as well.'"]}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": ["similar_path = get_image_paths(direc, indices_distances[0][0])[0]\n", "element = find_entry(data, 'image_path', similar_path)\n", "\n", "user_query = 'What is the capacity of this item?'\n", "prompt = f\"\"\"\n", "以下是用户查询，我根据提供的描述和图像来回答该查询。\n", "\n", "用户查询：\n", "{user_query}\n", "\n", "描述：\n", "{element['description']}\n", "\"\"\"\n", "image_query(prompt, similar_path)\n"]}, {"cell_type": "markdown", "metadata": {"id": "VIInamGaAG9L"}, "source": ["我们看到它能够回答这个问题。这只有通过直接匹配图像并从中收集相关描述作为上下文才能实现。\n"]}, {"cell_type": "markdown", "metadata": {"id": "Ljrf0VKR_2q9"}, "source": ["# 结论\n"]}, {"cell_type": "markdown", "metadata": {"id": "PexvxTF5_7ay"}, "source": ["在这个笔记本中，我们已经学习了如何使用CLIP模型，创建一个使用CLIP模型的图像嵌入数据库的示例，执行语义搜索，并最终提供用户查询来回答问题。\n"]}, {"cell_type": "markdown", "metadata": {"id": "gOgRBeh6eMiq"}, "source": ["这种使用模式的应用领域广泛，很容易进行改进以进一步增强技术。例如，您可以微调CLIP，可以改进像RAG中的检索过程，也可以引导工程GPT-V。\n"]}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.14"}}, "nbformat": 4, "nbformat_minor": 0}