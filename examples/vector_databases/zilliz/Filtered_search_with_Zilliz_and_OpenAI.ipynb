{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# 使用 Zilliz 和 OpenAI 进行筛选搜索\n", "### 寻找您的下一部电影\n", "\n", "在这个笔记本中，我们将讨论如何使用 OpenAI 生成电影描述的嵌入向量，并在 Zilliz 中使用这些嵌入向量来查找相关的电影。为了缩小搜索结果的范围并尝试一些新东西，我们将使用过滤器进行元数据搜索。本示例中的数据集来自 HuggingFace 数据集，包含8000多个电影条目。\n", "\n", "让我们首先下载本笔记本所需的库：\n", "- `openai` 用于与 OpenAI 嵌入服务进行通信\n", "- `pymilvus` 用于与 Zilliz 服务器进行通信\n", "- `datasets` 用于下载数据集\n", "- `tqdm` 用于显示进度条\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["! pip install openai pymilvus datasets tqdm\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["要使Zilliz运行起来，请查看[这里](https://zilliz.com/doc/quick_start)。在设置好您的账户和数据库之后，继续设置以下数值：\n", "- URI：您的数据库运行的URI\n", "- USER：您的数据库用户名\n", "- PASSWORD：您的数据库密码\n", "- COLLECTION_NAME：在Zilliz中命名集合的名称\n", "- DIMENSION：嵌入的维度\n", "- OPENAI_ENGINE：要使用的嵌入模型\n", "- openai.api_key：您的OpenAI账户密钥\n", "- INDEX_PARAM：用于集合的索引设置\n", "- QUERY_PARAM：要使用的搜索参数\n", "- BATCH_SIZE：一次要嵌入和插入多少个文本\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import openai\n", "\n", "URI = 'your_uri'\n", "TOKEN = 'your_token' # TOKEN == 用户:密码 或 api_key\n", "COLLECTION_NAME = 'book_search'\n", "DIMENSION = 1536\n", "OPENAI_ENGINE = 'text-embedding-3-small'\n", "openai.api_key = 'sk-your_key'\n", "\n", "INDEX_PARAM = {\n", "    'metric_type':'L2',\n", "    'index_type':\"AUTOINDEX\",\n", "    'params':{}\n", "}\n", "\n", "QUERY_PARAM = {\n", "    \"metric_type\": \"L2\",\n", "    \"params\": {},\n", "}\n", "\n", "BATCH_SIZE = 1000\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["from pymilvus import connections, utility, FieldSchema, Collection, CollectionSchema, DataType\n", "\n", "# 连接到Zilliz数据库\n", "connections.connect(uri=URI, token=TOKEN)\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# 如果集合已存在，则将其移除。\n", "if utility.has_collection(COLLECTION_NAME):\n", "    utility.drop_collection(COLLECTION_NAME)\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# 创建一个集合，包含id、标题和嵌入信息。\n", "fields = [\n", "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n", "    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=64000),\n", "    FieldSchema(name='type', dtype=DataType.VARCHAR, max_length=64000),\n", "    FieldSchema(name='release_year', dtype=DataType.INT64),\n", "    FieldSchema(name='rating', dtype=DataType.VARCHAR, max_length=64000),\n", "    FieldSchema(name='description', dtype=DataType.VARCHAR, max_length=64000),\n", "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n", "]\n", "schema = CollectionSchema(fields=fields)\n", "collection = Collection(name=COLLECTION_NAME, schema=schema)\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# 在集合上创建索引并加载它。\n", "collection.create_index(field_name=\"embedding\", index_params=INDEX_PARAM)\n", "collection.load()\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 数据集\n", "有了Zilliz运行起来，我们就可以开始获取我们的数据了。`Hugging Face Datasets` 是一个包含许多不同用户数据集的中心，而在这个示例中，我们使用了HuggingLearners的netflix-shows数据集。该数据集包含超过8000部电影的电影及其元数据对。我们将嵌入每个描述并将其与标题、类型、发布年份和评分一起存储在Zilliz中。\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/Users/filiphaltmayer/miniconda3/envs/haystack/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n", "  from .autonotebook import tqdm as notebook_tqdm\n", "Found cached dataset csv (/Users/filiphaltmayer/.cache/huggingface/datasets/hugginglearners___csv/hugginglearners--netflix-shows-03475319fc65a05a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"]}], "source": ["import datasets\n", "\n", "# 下载数据集 \n", "dataset = datasets.load_dataset('hugginglearners/netflix-shows', split='train')\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 插入数据\n", "现在我们已经将数据保存在我们的机器上，我们可以开始将其嵌入并插入到 Zilliz 中。嵌入函数接受文本并以列表格式返回嵌入。\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# 简单函数，用于将文本转换为嵌入表示。\n", "def embed(texts):\n", "    embeddings = openai.Embedding.create(\n", "        input=texts,\n", "        engine=OPENAI_ENGINE\n", "    )\n", "    return [x['embedding'] for x in embeddings['data']]\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["接下来的步骤是实际的插入操作。我们遍历所有条目，并创建批次，一旦达到设定的批次大小，我们就插入这些批次。循环结束后，如果存在剩余的最后一个批次，我们也会将其插入。\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|██████████| 8807/8807 [00:54<00:00, 162.59it/s]\n"]}], "source": ["from tqdm import tqdm\n", "\n", "data = [\n", "    [], # 标题\n", "    [], # 类型\n", "    [], # 发行年份\n", "    [], # 评级\n", "    [], # 描述\n", "]\n", "\n", "# 批量嵌入和插入\n", "for i in tqdm(range(0, len(dataset))):\n", "    data[0].append(dataset[i]['title'] or '')\n", "    data[1].append(dataset[i]['type'] or '')\n", "    data[2].append(dataset[i]['release_year'] or -1)\n", "    data[3].append(dataset[i]['rating'] or '')\n", "    data[4].append(dataset[i]['description'] or '')\n", "    if len(data[0]) % BATCH_SIZE == 0:\n", "        data.append(embed(data[4]))\n", "        collection.insert(data)\n", "        data = [[],[],[],[],[]]\n", "\n", "# 嵌入并插入余数 \n", "if len(data[0]) != 0:\n", "    data.append(embed(data[4]))\n", "    collection.insert(data)\n", "    data = [[],[],[],[],[]]\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 查询数据库\n", "在我们的数据安全地插入到 Zilliz 后，现在我们可以执行查询操作了。查询操作接受一个包含您要搜索的电影描述和要使用的过滤器的元组。有关过滤器的更多信息可以在[这里](https://milvus.io/docs/boolean.md)找到。搜索首先打印出您的描述和过滤器表达式。然后对于每个结果，我们打印出电影的得分、标题、类型、发行年份、评分和描述。\n"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Description: movie about a fluffly animal Expression: release_year < 2019 and rating like \"PG%\"\n", "Results:\n", "\tRank: 1 Score: 0.30085673928260803 Title: The Lamb\n", "\t\tType: Movie Release Year: 2017 Rating: PG\n", "A big-dreaming donkey escapes his menial existence and befriends some free-spirited\n", "animal pals in this imaginative retelling of the Nativity Story.\n", "\n", "\tRank: 2 Score: 0.3352621793746948 Title: Puss in Boots\n", "\t\tType: Movie Release Year: 2011 Rating: PG\n", "The fabled feline heads to the Land of Giants with friends Humpty Dumpty and Kitty\n", "Softpaws on a quest to nab its greatest treasure: the Golden Goose.\n", "\n", "\tRank: 3 Score: 0.3415083587169647 Title: Show Dogs\n", "\t\tType: Movie Release Year: 2018 Rating: PG\n", "A rough and tough police dog must go undercover with an FBI agent as a prim and proper\n", "pet at a dog show to save a baby panda from an illegal sale.\n", "\n", "\tRank: 4 Score: 0.3428957462310791 Title: Open Season 2\n", "\t\tType: Movie Release Year: 2008 Rating: PG\n", "Elliot the buck and his forest-dwelling cohorts must rescue their dachshund pal from\n", "some spoiled pets bent on returning him to domesticity.\n", "\n", "\tRank: 5 Score: 0.34376364946365356 Title: Stuart Little 2\n", "\t\tType: Movie Release Year: 2002 Rating: PG\n", "Zany misadventures are in store as lovable city mouse Stuart and his human brother,\n", "George, raise the roof in this sequel to the 1999 blockbuster.\n", "\n"]}], "source": ["import textwrap\n", "\n", "def query(query, top_k = 5):\n", "    text, expr = query\n", "    res = collection.search(embed(text), anns_field='embedding', expr = expr, param=QUERY_PARAM, limit = top_k, output_fields=['title', 'type', 'release_year', 'rating', 'description'])\n", "    for i, hit in enumerate(res):\n", "        print('Description:', text, 'Expression:', expr)\n", "        print('Results:')\n", "        for ii, hits in enumerate(hit):\n", "            print('\\t' + 'Rank:', ii + 1, 'Score:', hits.score, 'Title:', hits.entity.get('title'))\n", "            print('\\t\\t' + 'Type:', hits.entity.get('type'), 'Release Year:', hits.entity.get('release_year'), 'Rating:', hits.entity.get('rating'))\n", "            print(textwrap.fill(hits.entity.get('description'), 88))\n", "            print()\n", "\n", "my_query = ('movie about a fluffly animal', 'release_year < 2019 and rating like \\\"PG%\\\"')\n", "\n", "query(my_query)\n"]}], "metadata": {"kernelspec": {"display_name": "haystack", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.16"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}