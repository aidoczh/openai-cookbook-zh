{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 使用Tair作为OpenAI嵌入向量数据库\n", "\n", "本笔记本将逐步指导您如何将Tair用作OpenAI嵌入向量数据库。\n", "\n", "本笔记本展示了以下端到端的过程：\n", "1. 使用OpenAI API创建的预先计算的嵌入向量。\n", "2. 将嵌入向量存储在Tair的云实例中。\n", "3. 将原始文本查询转换为使用OpenAI API的嵌入向量。\n", "4. 使用Tair在创建的集合中执行最近邻搜索。\n", "\n", "### 什么是Tair\n", "\n", "[Tair](https://www.alibabacloud.com/help/en/tair/latest/what-is-tair) 是由阿里云开发的云原生内存数据库服务。Tair兼容开源Redis，并提供各种数据模型和企业级功能，以支持您的实时在线场景。Tair还推出了基于新型非易失性内存（NVM）存储介质的持久内存优化实例。这些实例可以降低成本约30%，确保数据持久性，并提供几乎与内存数据库相同的性能。Tair已被广泛应用于政务、金融、制造业、医疗保健和泛互联网等领域，以满足其高速查询和计算需求。\n", "\n", "[Tairvector](https://www.alibabacloud.com/help/en/tair/latest/tairvector) 是一种内部数据结构，提供高性能的实时向量存储和检索。TairVector提供两种索引算法：分层可导航小世界（HNSW）和平面搜索。此外，TairVector支持多种距离函数，如欧氏距离、内积和Jaccard距离。与传统的向量检索服务相比，TairVector具有以下优势：\n", "- 将所有数据存储在内存中，并支持实时索引更新，以减少读写操作的延迟。\n", "- 使用优化的内存数据结构更好地利用存储容量。\n", "- 作为一个开箱即用的数据结构，在简单高效的架构中运行，无需复杂的模块或依赖项。\n", "\n", "### 部署选项\n", "\n", "- 使用[Tair云向量数据库](https://www.alibabacloud.com/help/en/tair/latest/getting-started-overview)。[单击此处](https://www.alibabacloud.com/product/tair)快速部署。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 先决条件\n", "\n", "为了完成这个练习，我们需要准备一些事项：\n", "\n", "1. Tair 云服务器实例。\n", "2. 用于与 tair 数据库交互的 'tair' 库。\n", "3. 一个[OpenAI API密钥](https://beta.openai.com/account/api-keys)。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 安装要求\n", "\n", "这个笔记本显然需要`openai`和`tair`包，但我们还会使用一些其他附加库。以下命令会安装它们全部：\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:05:05.718972Z", "start_time": "2023-02-16T12:04:30.434820Z"}, "pycharm": {"is_executing": true}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Looking in indexes: http://sg.mirrors.cloud.aliyuncs.com/pypi/simple/\n", "Requirement already satisfied: openai in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (0.28.0)\n", "Requirement already satisfied: redis in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (5.0.0)\n", "Requirement already satisfied: tair in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (1.3.6)\n", "Requirement already satisfied: pandas in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (2.1.0)\n", "Requirement already satisfied: wget in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (3.2)\n", "Requirement already satisfied: requests>=2.20 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from openai) (2.31.0)\n", "Requirement already satisfied: tqdm in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from openai) (4.66.1)\n", "Requirement already satisfied: aiohttp in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from openai) (3.8.5)\n", "Requirement already satisfied: async-timeout>=4.0.2 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from redis) (4.0.3)\n", "Requirement already satisfied: numpy>=1.22.4 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from pandas) (1.25.2)\n", "Requirement already satisfied: python-dateutil>=2.8.2 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from pandas) (2.8.2)\n", "Requirement already satisfied: pytz>=2020.1 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n", "Requirement already satisfied: tzdata>=2022.1 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from pandas) (2023.3)\n", "Requirement already satisfied: six>=1.5 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n", "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from requests>=2.20->openai) (3.2.0)\n", "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.4)\n", "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.7.22)\n", "Requirement already satisfied: attrs>=17.3.0 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from aiohttp->openai) (22.1.0)\n", "Requirement already satisfied: multidict<7.0,>=4.5 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n", "Requirement already satisfied: yarl<2.0,>=1.0 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n", "Requirement already satisfied: frozenlist>=1.1.1 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from aiohttp->openai) (1.4.0)\n", "Requirement already satisfied: aiosignal>=1.1.2 in /root/anaconda3/envs/notebook/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n", "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n", "\u001b[0m"]}], "source": ["! pip install openai redis tair pandas wget\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 准备你的OpenAI API密钥\n", "\n", "OpenAI API密钥用于对文档和查询进行向量化。\n", "\n", "如果你还没有OpenAI API密钥，可以从[https://beta.openai.com/account/api-keys](https://beta.openai.com/account/api-keys)获取。\n", "\n", "获取到密钥后，请通过getpass添加。\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:05:05.730338Z", "start_time": "2023-02-16T12:05:05.723351Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Input your OpenAI API key:········\n"]}], "source": ["import getpass\n", "import openai\n", "\n", "openai.api_key = getpass.getpass(\"Input your OpenAI API key:\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 连接到Tair\n", "首先将其添加到您的环境变量中。\n", "\n", "使用官方Python库连接到正在运行的Tair服务器实例非常简单。\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Input your tair url:········\n"]}], "source": ["# URL 格式：redis://[[username]:[password]]@localhost:6379/0\n", "TAIR_URL = getpass.getpass(\"Input your tair url:\")\n"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["from tair import Tair as TairClient\n", "\n", "# 从URL连接到Tair并创建客户端\n", "\n", "url = TAIR_URL\n", "client = TairClient.from_url(url)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以通过ping命令测试连接：\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:05:06.848488Z", "start_time": "2023-02-16T12:05:06.832612Z"}, "pycharm": {"is_executing": true}}, "outputs": [{"data": {"text/plain": ["True"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["client.ping()\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:05:37.371951Z", "start_time": "2023-02-16T12:05:06.851634Z"}, "pycharm": {"is_executing": true}, "scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["100% [......................................................................] 698933052 / 698933052"]}, {"data": {"text/plain": ["'vector_database_wikipedia_articles_embedded (1).zip'"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["import wget\n", "\n", "embeddings_url = \"https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip\"\n", "\n", "# 文件大小约为700MB，因此需要一些时间来完成。\n", "wget.download(embeddings_url)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["下载的文件必须被解压缩：\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:06:01.538851Z", "start_time": "2023-02-16T12:05:37.376042Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The file vector_database_wikipedia_articles_embedded.csv exists in the data directory.\n"]}], "source": ["import zipfile\n", "import os\n", "import re\n", "import tempfile\n", "\n", "current_directory = os.getcwd()\n", "zip_file_path = os.path.join(current_directory, \"vector_database_wikipedia_articles_embedded.zip\")\n", "output_directory = os.path.join(current_directory, \"../../data\")\n", "\n", "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n", "    zip_ref.extractall(output_directory)\n", "\n", "\n", "# 检查CSV文件是否存在\n", "file_name = \"vector_database_wikipedia_articles_embedded.csv\"\n", "data_directory = os.path.join(current_directory, \"../../data\")\n", "file_path = os.path.join(data_directory, file_name)\n", "\n", "\n", "if os.path.exists(file_path):\n", "    print(f\"The file {file_name} exists in the data directory.\")\n", "else:\n", "    print(f\"The file {file_name} does not exist in the data directory.\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 创建索引\n", "\n", "Tair将数据存储在索引中，其中每个对象由一个键描述。每个键包含一个向量和多个属性键。\n", "\n", "我们将首先创建两个索引，一个用于**title_vector**，另一个用于**content_vector**，然后我们将用预先计算的嵌入填充它。\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Index already exists\n", "Index already exists\n"]}], "source": ["# 设置索引参数\n", "index = \"openai_test\"\n", "embedding_dim = 1536\n", "distance_type = \"L2\"\n", "index_type = \"HNSW\"\n", "data_type = \"FLOAT32\"\n", "\n", "# 创建两个索引，一个用于title_vector，另一个用于content_vector，如果已存在则跳过。\n", "index_names = [index + \"_title_vector\", index+\"_content_vector\"]\n", "for index_name in index_names:\n", "    index_connection = client.tvs_get_index(index_name)\n", "    if index_connection is not None:\n", "        print(\"Index already exists\")\n", "    else:\n", "        client.tvs_create_index(name=index_name, dim=embedding_dim, distance_type=distance_type,\n", "                                index_type=index_type, data_type=data_type)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 加载数据\n", "\n", "在本节中，我们将加载在本次会话之前准备好的数据，这样您就不必使用自己的学分重新计算维基百科文章的嵌入。\n"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from ast import literal_eval\n", "# 本地CSV文件的路径\n", "csv_file_path = '../../data/vector_database_wikipedia_articles_embedded.csv'\n", "article_df = pd.read_csv(csv_file_path)\n", "\n", "# 从字符串中读取向量并将其转换为列表\n", "article_df['title_vector'] = article_df.title_vector.apply(literal_eval).values\n", "article_df['content_vector'] = article_df.content_vector.apply(literal_eval).values\n", "\n", "# 向索引添加/更新数据\n", "for i in range(len(article_df)):\n", "    # 将数据添加到索引，包含标题向量\n", "    client.tvs_hset(index=index_names[0], key=article_df.id[i].item(), vector=article_df.title_vector[i], is_binary=False,\n", "                    **{\"url\": article_df.url[i], \"title\": article_df.title[i], \"text\": article_df.text[i]})\n", "    # 将数据添加到索引并包含content_vector\n", "    client.tvs_hset(index=index_names[1], key=article_df.id[i].item(), vector=article_df.content_vector[i], is_binary=False,\n", "                    **{\"url\": article_df.url[i], \"title\": article_df.title[i], \"text\": article_df.text[i]})\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:30:40.675202Z", "start_time": "2023-02-16T12:30:40.655654Z"}, "pycharm": {"is_executing": true}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Count in openai_test_title_vector:25000\n", "Count in openai_test_content_vector:25000\n"]}], "source": ["# 检查数据计数，确保所有点都已存储。\n", "for index_name in index_names:\n", "    stats = client.tvs_get_index(index_name)\n", "    count = int(stats[\"current_record_count\"]) - int(stats[\"delete_record_count\"])\n", "    print(f\"Count in {index_name}:{count}\")\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 搜索数据\n", "\n", "一旦数据被放入Tair中，我们将开始查询集合中最接近的向量。我们可以提供一个额外的参数`vector_name`，以从基于标题的搜索切换到基于内容的搜索。由于预先计算的嵌入是使用`text-embedding-3-small` OpenAI模型创建的，因此我们在搜索过程中也必须使用它。\n"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:30:38.024370Z", "start_time": "2023-02-16T12:30:37.712816Z"}}, "outputs": [], "source": ["def query_tair(client, query, vector_name=\"title_vector\", top_k=5):\n", "\n", "    # 从用户查询生成嵌入向量\n", "    embedded_query = openai.Embedding.create(\n", "        input= query,\n", "        model=\"text-embedding-3-small\",\n", "    )[\"data\"][0]['embedding']\n", "    embedded_query = np.array(embedded_query)\n", "\n", "    # 在索引中搜索向量最接近的k个近似最近邻\n", "    query_result = client.tvs_knnsearch(index=index+\"_\"+vector_name, k=top_k, vector=embedded_query)\n", "\n", "    return query_result\n"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:30:39.379566Z", "start_time": "2023-02-16T12:30:38.031041Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1. Museum of Modern Art (Distance: 0.125)\n", "2. Western Europe (Distance: 0.133)\n", "3. Renaissance art (Distance: 0.136)\n", "4. Pop art (Distance: 0.14)\n", "5. Northern Europe (Distance: 0.145)\n"]}], "source": ["import openai\n", "import numpy as np\n", "\n", "query_result = query_tair(client=client, query=\"modern art in Europe\", vector_name=\"title_vector\")\n", "for i in range(len(query_result)):\n", "    title = client.tvs_hmget(index+\"_\"+\"content_vector\", query_result[i][0].decode('utf-8'), \"title\")\n", "    print(f\"{i + 1}. {title[0].decode('utf-8')} (Distance: {round(query_result[i][1],3)})\")\n"]}, {"cell_type": "code", "execution_count": 17, "metadata": {"ExecuteTime": {"end_time": "2023-02-16T12:30:40.652676Z", "start_time": "2023-02-16T12:30:39.382555Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1. Battle of Bannockburn (Distance: 0.131)\n", "2. Wars of Scottish Independence (Distance: 0.139)\n", "3. 1651 (Distance: 0.147)\n", "4. First War of Scottish Independence (Distance: 0.15)\n", "5. Robert I of Scotland (Distance: 0.154)\n"]}], "source": ["# This time we'll query using content vector\n", "query_result = query_tair(client=client, query=\"Famous battles in Scottish history\", vector_name=\"content_vector\")\n", "for i in range(len(query_result)):\n", "    title = client.tvs_hmget(index+\"_\"+\"content_vector\", query_result[i][0].decode('utf-8'), \"title\")\n", "    print(f\"{i + 1}. {title[0].decode('utf-8')} (Distance: {round(query_result[i][1],3)})\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n"]}], "metadata": {"kernelspec": {"display_name": "Python [conda env:notebook] *", "language": "python", "name": "conda-env-notebook-py"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 1}